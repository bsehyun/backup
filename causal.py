# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import numpy as np
import pandas as pd
from causallearn.search.ConstraintBased.PC import pc
from causallearn.search.ScoreBased.GES import ges
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import PC as pgmpy_PC, HillClimbSearch, BicScore
import dowhy
from dowhy import CausalModel
import shap
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.impute import KNNImputer
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ACN ê³µì • ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def preprocess_acn_data(raw_data, visualize=True):
    """
    ACN ê³µì • ë°ì´í„° ì „ì²˜ë¦¬ - ì´ìƒì¹˜ ì œê±°, ì •ê·œí™”, ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("=" * 60)
    print("ACN ê³µì • ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)
    
    # 1. ì´ìƒì¹˜ ì œê±° (í™”í•™ê³µì • íŠ¹í™”)
    def detect_process_outliers(data, columns, method='IQR'):
        outlier_mask = pd.Series([False] * len(data))
        outlier_info = {}
        
        for col in columns:
            if method == 'IQR':
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower = Q1 - 1.5 * IQR
                upper = Q3 + 1.5 * IQR
                col_outliers = (data[col] < lower) | (data[col] > upper)
                outlier_mask |= col_outliers
                outlier_info[col] = {
                    'count': col_outliers.sum(),
                    'percentage': (col_outliers.sum() / len(data)) * 100,
                    'bounds': (lower, upper)
                }
        
        return ~outlier_mask, outlier_info
    
    # 2. ê³µì • ë³€ìˆ˜ë³„ ì •ê·œí™”
    process_vars = ['temperature', 'pressure', 'nh3_c3h6_ratio', 
                   'o2_c3h6_ratio', 'ghsv', 'catalyst_age']
    
    # ì´ìƒì¹˜ íƒì§€ ë° ì œê±°
    clean_mask, outlier_info = detect_process_outliers(raw_data, process_vars)
    clean_data = raw_data[clean_mask].copy()
    
    # ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼:")
    print(f"   - ì›ë³¸ ë°ì´í„°: {len(raw_data)}ê°œ ìƒ˜í”Œ")
    print(f"   - ì´ìƒì¹˜ ì œê±° í›„: {len(clean_data)}ê°œ ìƒ˜í”Œ")
    print(f"   - ì œê±°ëœ ìƒ˜í”Œ: {len(raw_data) - len(clean_data)}ê°œ ({((len(raw_data) - len(clean_data))/len(raw_data)*100):.1f}%)")
    
    print(f"\nğŸ“ˆ ë³€ìˆ˜ë³„ ì´ìƒì¹˜ í˜„í™©:")
    for var, info in outlier_info.items():
        print(f"   - {var}: {info['count']}ê°œ ({info['percentage']:.1f}%)")
    
    # ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, var in enumerate(process_vars):
            if i < len(axes):
                # ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ì´ìƒì¹˜ ì‹œê°í™”
                axes[i].boxplot([raw_data[var], clean_data[var]], 
                               labels=['Original', 'Cleaned'])
                axes[i].set_title(f'{var} - Outlier Detection')
                axes[i].set_ylabel('Value')
                axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.suptitle('ACN Process Data - Outlier Detection Results', fontsize=16, y=1.02)
        plt.show()
    
    # 3. Robust Scaling (ê³µì • ë°ì´í„° íŠ¹ì„± ê³ ë ¤)
    scaler = RobustScaler()
    clean_data[process_vars] = scaler.fit_transform(clean_data[process_vars])
    
    print(f"\nğŸ”§ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ (RobustScaler ì ìš©)")
    
    # 4. KNN ê²°ì¸¡ì¹˜ ë³´ê°„
    missing_before = clean_data[process_vars].isnull().sum().sum()
    imputer = KNNImputer(n_neighbors=5)
    clean_data[process_vars] = imputer.fit_transform(clean_data[process_vars])
    missing_after = clean_data[process_vars].isnull().sum().sum()
    
    print(f"\nğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬:")
    print(f"   - ì²˜ë¦¬ ì „ ê²°ì¸¡ì¹˜: {missing_before}ê°œ")
    print(f"   - ì²˜ë¦¬ í›„ ê²°ì¸¡ì¹˜: {missing_after}ê°œ")
    
    # ì „ì²˜ë¦¬ í›„ ë°ì´í„° ë¶„í¬ ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, var in enumerate(process_vars):
            if i < len(axes):
                axes[i].hist(clean_data[var], bins=30, alpha=0.7, edgecolor='black')
                axes[i].set_title(f'{var} - Normalized Distribution')
                axes[i].set_xlabel('Normalized Value')
                axes[i].set_ylabel('Frequency')
                axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.suptitle('ACN Process Data - After Preprocessing', fontsize=16, y=1.02)
        plt.show()
    
    print(f"\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   - ìµœì¢… ë°ì´í„° í¬ê¸°: {clean_data.shape}")
    print(f"   - ì²˜ë¦¬ëœ ë³€ìˆ˜: {len(process_vars)}ê°œ")
    
    return clean_data, scaler, outlier_info

# VIF ê¸°ë°˜ ë‹¤ì¤‘ê³µì„ ì„± ì²˜ë¦¬ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def handle_multicollinearity(df, threshold=5, visualize=True):
    """
    VIF ì„ê³„ê°’ ê¸°ë°˜ ë‹¤ì¤‘ê³µì„ ì„± ë³€ìˆ˜ ì œê±°
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì‹œì‘ (VIF ê¸°ë°˜)")
    print("=" * 60)
    
    def calculate_vif(df):
        vif_data = pd.DataFrame()
        vif_data["feature"] = df.columns
        vif_data["VIF"] = [variance_inflation_factor(df.values, i)
                          for i in range(len(df.columns))]
        return vif_data
    
    # ì´ˆê¸° VIF ê³„ì‚°
    initial_vif = calculate_vif(df)
    print(f"\nğŸ“Š ì´ˆê¸° VIF ë¶„ì„ ê²°ê³¼:")
    print(f"   - ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜: {len(df.columns)}ê°œ")
    print(f"   - VIF ì„ê³„ê°’: {threshold}")
    
    # VIF ì‹œê°í™”
    if visualize:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ì´ˆê¸° VIF ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax1.bar(range(len(initial_vif)), initial_vif['VIF'], 
                      color=['red' if vif > threshold else 'green' for vif in initial_vif['VIF']])
        ax1.set_xlabel('Variables')
        ax1.set_ylabel('VIF Value')
        ax1.set_title('Initial VIF Analysis')
        ax1.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')
        ax1.set_xticks(range(len(initial_vif)))
        ax1.set_xticklabels(initial_vif['feature'], rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # VIF ê°’ ë¼ë²¨ ì¶”ê°€
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.1f}', ha='center', va='bottom', fontsize=8)
    
    features_to_keep = df.columns.tolist()
    removed_features = []
    iteration = 0
    
    print(f"\nğŸ”„ ë‹¤ì¤‘ê³µì„ ì„± ì œê±° ê³¼ì •:")
    
    while True:
        iteration += 1
        vif_df = calculate_vif(df[features_to_keep])
        max_vif = vif_df['VIF'].max()
        
        if max_vif <= threshold:
            break
            
        feature_to_remove = vif_df.loc[vif_df['VIF'].idxmax(), 'feature']
        features_to_keep.remove(feature_to_remove)
        removed_features.append((feature_to_remove, max_vif, iteration))
        print(f"   Iteration {iteration}: {feature_to_remove} ì œê±° (VIF: {max_vif:.2f})")
    
    # ìµœì¢… ê²°ê³¼
    final_vif = calculate_vif(df[features_to_keep])
    
    print(f"\nâœ… ë‹¤ì¤‘ê³µì„ ì„± ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ì œê±°ëœ ë³€ìˆ˜: {len(removed_features)}ê°œ")
    print(f"   - ë‚¨ì€ ë³€ìˆ˜: {len(features_to_keep)}ê°œ")
    print(f"   - ìµœëŒ€ VIF: {final_vif['VIF'].max():.2f}")
    
    if removed_features:
        print(f"\nğŸ“‹ ì œê±°ëœ ë³€ìˆ˜ ëª©ë¡:")
        for feature, vif, iter_num in removed_features:
            print(f"   - {feature} (VIF: {vif:.2f}, Iteration: {iter_num})")
    
    # ìµœì¢… VIF ì‹œê°í™”
    if visualize:
        # ìµœì¢… VIF ë§‰ëŒ€ ê·¸ë˜í”„
        bars2 = ax2.bar(range(len(final_vif)), final_vif['VIF'], 
                       color=['green' if vif <= threshold else 'orange' for vif in final_vif['VIF']])
        ax2.set_xlabel('Variables')
        ax2.set_ylabel('VIF Value')
        ax2.set_title('Final VIF Analysis (After Removal)')
        ax2.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')
        ax2.set_xticks(range(len(final_vif)))
        ax2.set_xticklabels(final_vif['feature'], rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # VIF ê°’ ë¼ë²¨ ì¶”ê°€
        for i, bar in enumerate(bars2):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.1f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        plt.suptitle('Multicollinearity Analysis (VIF)', fontsize=16, y=1.02)
        plt.show()
        
        # ì œê±° ê³¼ì • íˆíŠ¸ë§µ
        if removed_features:
            fig, ax = plt.subplots(figsize=(10, 6))
            removal_data = pd.DataFrame(removed_features, 
                                      columns=['Feature', 'VIF', 'Iteration'])
            
            # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
            heatmap_data = np.zeros((len(removed_features), 3))
            for i, (_, vif, iter_num) in enumerate(removed_features):
                heatmap_data[i, 0] = vif
                heatmap_data[i, 1] = iter_num
                heatmap_data[i, 2] = 1  # ì œê±°ë¨ì„ í‘œì‹œ
            
            im = ax.imshow(heatmap_data.T, cmap='Reds', aspect='auto')
            ax.set_xticks(range(len(removed_features)))
            ax.set_xticklabels(removal_data['Feature'], rotation=45, ha='right')
            ax.set_yticks(range(3))
            ax.set_yticklabels(['VIF Value', 'Iteration', 'Removed'])
            ax.set_title('Feature Removal Process')
            
            # ê°’ ë¼ë²¨ ì¶”ê°€
            for i in range(len(removed_features)):
                ax.text(i, 0, f'{removal_data.iloc[i]["VIF"]:.1f}', 
                       ha='center', va='center', fontweight='bold')
                ax.text(i, 1, f'{removal_data.iloc[i]["Iteration"]}', 
                       ha='center', va='center', fontweight='bold')
            
            plt.colorbar(im, ax=ax)
            plt.tight_layout()
            plt.show()
    
    return features_to_keep, removed_features, final_vif



# PC ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì¸ê³¼êµ¬ì¡° ë°œê²¬ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def discover_causal_structure_pc(data, alpha=0.05, visualize=True):
    """
    PC ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ACN ê³µì •ì˜ ì¸ê³¼êµ¬ì¡° ë°œê²¬
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("PC ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì¸ê³¼êµ¬ì¡° ë°œê²¬")
    print("=" * 60)
    
    print(f"\nğŸ” PC ì•Œê³ ë¦¬ì¦˜ ì„¤ì •:")
    print(f"   - ìœ ì˜ìˆ˜ì¤€ (alpha): {alpha}")
    print(f"   - ë…ë¦½ì„± ê²€ì •: Fisher's Z")
    print(f"   - ë¶„ì„ ë³€ìˆ˜: {len(data.columns)}ê°œ")
    print(f"   - ìƒ˜í”Œ ìˆ˜: {len(data)}ê°œ")
    
    # PC ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
    print(f"\nâš™ï¸ PC ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì¤‘...")
    cg = pc(data.values, alpha=alpha, indep_test='fisherz', 
            stable=True, uc_rule=0)
    
    # NetworkX ê·¸ë˜í”„ë¡œ ë³€í™˜
    G = nx.DiGraph()
    node_names = data.columns.tolist()
    
    # ë…¸ë“œ ì¶”ê°€
    G.add_nodes_from(node_names)
    
    # ì—£ì§€ ì¶”ê°€ (adjacency matrix ê¸°ë°˜)
    adj_matrix = cg.G.graph
    edges_found = []
    
    for i in range(len(node_names)):
        for j in range(len(node_names)):
            if adj_matrix[i, j] == 1:
                G.add_edge(node_names[i], node_names[j])
                edges_found.append((node_names[i], node_names[j]))
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š ì¸ê³¼êµ¬ì¡° ë°œê²¬ ê²°ê³¼:")
    print(f"   - ë°œê²¬ëœ ì¸ê³¼ê´€ê³„: {len(edges_found)}ê°œ")
    print(f"   - ë…¸ë“œ ìˆ˜: {len(G.nodes())}ê°œ")
    print(f"   - ì—°ê²° ë°€ë„: {nx.density(G):.3f}")
    
    if edges_found:
        print(f"\nğŸ”— ë°œê²¬ëœ ì¸ê³¼ê´€ê³„:")
        for i, (source, target) in enumerate(edges_found, 1):
            print(f"   {i}. {source} â†’ {target}")
    
    # ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ë¶„ì„
    centrality_measures = {
        'degree': nx.degree_centrality(G),
        'betweenness': nx.betweenness_centrality(G),
        'closeness': nx.closeness_centrality(G),
        'eigenvector': nx.eigenvector_centrality(G, max_iter=1000)
    }
    
    print(f"\nğŸ“ˆ ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ì„± ë¶„ì„:")
    for measure_name, centrality in centrality_measures.items():
        if centrality:
            most_central = max(centrality, key=centrality.get)
            print(f"   - {measure_name} ì¤‘ì‹¬ì„± ìµœê³ : {most_central} ({centrality[most_central]:.3f})")
    
    # ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ì¸ê³¼ ê·¸ë˜í”„ ì‹œê°í™”
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        nx.draw_networkx(G, pos, ax=axes[0,0], 
                        node_color='lightblue', 
                        node_size=2000,
                        font_size=10, 
                        arrows=True,
                        arrowsize=20,
                        edge_color='gray',
                        with_labels=True)
        axes[0,0].set_title('Causal DAG Structure (PC Algorithm)', fontsize=14, fontweight='bold')
        axes[0,0].axis('off')
        
        # 2. ì¤‘ì‹¬ì„± ë¹„êµ
        centrality_df = pd.DataFrame(centrality_measures)
        if not centrality_df.empty:
            centrality_df.plot(kind='bar', ax=axes[0,1], width=0.8)
            axes[0,1].set_title('Network Centrality Measures', fontsize=14, fontweight='bold')
            axes[0,1].set_xlabel('Variables')
            axes[0,1].set_ylabel('Centrality Score')
            axes[0,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            axes[0,1].tick_params(axis='x', rotation=45)
            axes[0,1].grid(True, alpha=0.3)
        
        # 3. ì¸ì ‘ í–‰ë ¬ íˆíŠ¸ë§µ
        adj_df = pd.DataFrame(adj_matrix, 
                             index=node_names, 
                             columns=node_names)
        sns.heatmap(adj_df, annot=True, cmap='Blues', 
                   ax=axes[1,0], cbar_kws={'label': 'Connection'})
        axes[1,0].set_title('Adjacency Matrix', fontsize=14, fontweight='bold')
        axes[1,0].set_xlabel('Target Variables')
        axes[1,0].set_ylabel('Source Variables')
        
        # 4. ë„¤íŠ¸ì›Œí¬ í†µê³„
        network_stats = {
            'Nodes': len(G.nodes()),
            'Edges': len(G.edges()),
            'Density': nx.density(G),
            'Avg Clustering': nx.average_clustering(G.to_undirected()) if len(G.edges()) > 0 else 0,
            'Connected Components': nx.number_connected_components(G.to_undirected())
        }
        
        stats_df = pd.DataFrame(list(network_stats.items()), 
                               columns=['Metric', 'Value'])
        axes[1,1].axis('off')
        
        # í…Œì´ë¸” í˜•íƒœë¡œ í†µê³„ í‘œì‹œ
        table_data = []
        for metric, value in network_stats.items():
            table_data.append([metric, f'{value:.3f}' if isinstance(value, float) else str(value)])
        
        table = axes[1,1].table(cellText=table_data,
                               colLabels=['Network Metric', 'Value'],
                               cellLoc='center',
                               loc='center',
                               bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1, 2)
        axes[1,1].set_title('Network Statistics', fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.suptitle('PC Algorithm - Causal Structure Discovery', fontsize=16, y=1.02)
        plt.show()
        
        # ì¶”ê°€: ì¸ê³¼ê´€ê³„ ê°•ë„ ë¶„ì„ (ìƒê´€ê³„ìˆ˜ ê¸°ë°˜)
        if edges_found:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            edge_strengths = []
            edge_labels = []
            
            for source, target in edges_found:
                if source in data.columns and target in data.columns:
                    corr = data[source].corr(data[target])
                    edge_strengths.append(abs(corr))
                    edge_labels.append(f"{source}â†’{target}\n(r={corr:.3f})")
            
            if edge_strengths:
                bars = ax.bar(range(len(edge_strengths)), edge_strengths, 
                             color='skyblue', edgecolor='navy', alpha=0.7)
                ax.set_xlabel('Causal Relationships')
                ax.set_ylabel('Correlation Strength (|r|)')
                ax.set_title('Causal Relationship Strengths', fontsize=14, fontweight='bold')
                ax.set_xticks(range(len(edge_labels)))
                ax.set_xticklabels(edge_labels, rotation=45, ha='right')
                ax.grid(True, alpha=0.3)
                
                # ê°’ ë¼ë²¨ ì¶”ê°€
                for i, bar in enumerate(bars):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                plt.show()
    
    print(f"\nâœ… PC ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì™„ë£Œ!")
    print(f"   - ì¸ê³¼êµ¬ì¡° ë°œê²¬: {len(edges_found)}ê°œ ê´€ê³„")
    print(f"   - ë„¤íŠ¸ì›Œí¬ ë°€ë„: {nx.density(G):.3f}")
    
    return G, cg, centrality_measures, edges_found

# DoWhy í”„ë ˆì„ì›Œí¬ í™œìš©í•œ ì¸ê³¼íš¨ê³¼ ì¶”ì • (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def estimate_causal_effect_dowhy(data, treatment, outcome, common_causes, visualize=True):
    """
    DoWhyë¥¼ ì´ìš©í•œ ì²´ê³„ì  ì¸ê³¼íš¨ê³¼ ì¶”ì •
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("DoWhy í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ ì¸ê³¼íš¨ê³¼ ì¶”ì •")
    print("=" * 60)
    
    print(f"\nğŸ¯ ì¸ê³¼íš¨ê³¼ ë¶„ì„ ì„¤ì •:")
    print(f"   - ì²˜ë¦¬ ë³€ìˆ˜ (Treatment): {treatment}")
    print(f"   - ê²°ê³¼ ë³€ìˆ˜ (Outcome): {outcome}")
    print(f"   - ê³µí†µ ì›ì¸ (Common Causes): {common_causes}")
    print(f"   - ë°ì´í„° í¬ê¸°: {data.shape}")
    
    # 1. ì¸ê³¼ ëª¨ë¸ êµ¬ì„±
    print(f"\nğŸ”§ ì¸ê³¼ ëª¨ë¸ êµ¬ì„± ì¤‘...")
    model = CausalModel(
        data=data,
        treatment=treatment,
        outcome=outcome,
        common_causes=common_causes,
        effect_modifiers=None
    )
    
    # 2. ì‹ë³„ (Identification)
    print(f"\nğŸ” ì¸ê³¼íš¨ê³¼ ì‹ë³„ ì¤‘...")
    identified_estimand = model.identify_effect()
    print(f"   - ì‹ë³„ëœ ì¶”ì •ëŸ‰: {identified_estimand}")
    
    # 3. ì¶”ì • (Estimation) - ì—¬ëŸ¬ ë°©ë²• ì‹œë„
    print(f"\nğŸ“Š ì¸ê³¼íš¨ê³¼ ì¶”ì • ì¤‘...")
    estimates = {}
    estimate_details = {}
    
    # Backdoor criterion
    try:
        backdoor_estimate = model.estimate_effect(identified_estimand,
                                                 method_name="backdoor.linear_regression")
        estimates['backdoor'] = backdoor_estimate.value
        estimate_details['backdoor'] = {
            'value': backdoor_estimate.value,
            'confidence_interval': backdoor_estimate.confidence_interval,
            'p_value': getattr(backdoor_estimate, 'p_value', None)
        }
        print(f"   âœ… Backdoor ì¶”ì •: {backdoor_estimate.value:.4f}")
    except Exception as e:
        estimates['backdoor'] = None
        print(f"   âŒ Backdoor ì¶”ì • ì‹¤íŒ¨: {str(e)}")
    
    # Instrumental variables
    try:
        iv_estimate = model.estimate_effect(identified_estimand,
                                           method_name="iv.instrumental_variable")
        estimates['iv'] = iv_estimate.value
        estimate_details['iv'] = {
            'value': iv_estimate.value,
            'confidence_interval': iv_estimate.confidence_interval,
            'p_value': getattr(iv_estimate, 'p_value', None)
        }
        print(f"   âœ… IV ì¶”ì •: {iv_estimate.value:.4f}")
    except Exception as e:
        estimates['iv'] = None
        print(f"   âŒ IV ì¶”ì • ì‹¤íŒ¨: {str(e)}")
    
    # 4. ë°˜ë°•ê²€ì‚¬ (Refutation)
    print(f"\nğŸ§ª ë°˜ë°•ê²€ì‚¬ ì‹¤í–‰ ì¤‘...")
    refutation_results = {}
    
    # Random common cause
    try:
        if 'backdoor' in estimates and estimates['backdoor'] is not None:
            refute_random = model.refute_estimate(identified_estimand, backdoor_estimate,
                                                method_name="random_common_cause")
            refutation_results['random_common_cause'] = refute_random.new_effect
            print(f"   âœ… Random Common Cause: {refute_random.new_effect:.4f}")
    except Exception as e:
        print(f"   âŒ Random Common Cause ì‹¤íŒ¨: {str(e)}")
    
    # Placebo treatment
    try:
        if 'backdoor' in estimates and estimates['backdoor'] is not None:
            refute_placebo = model.refute_estimate(identified_estimand, backdoor_estimate,
                                                 method_name="placebo_treatment_refuter")
            refutation_results['placebo_treatment'] = refute_placebo.new_effect
            print(f"   âœ… Placebo Treatment: {refute_placebo.new_effect:.4f}")
    except Exception as e:
        print(f"   âŒ Placebo Treatment ì‹¤íŒ¨: {str(e)}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“‹ ì¸ê³¼íš¨ê³¼ ì¶”ì • ê²°ê³¼ ìš”ì•½:")
    for method, value in estimates.items():
        if value is not None:
            print(f"   - {method}: {value:.4f}")
        else:
            print(f"   - {method}: ì¶”ì • ì‹¤íŒ¨")
    
    # ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ì¸ê³¼íš¨ê³¼ ì¶”ì •ê°’ ë¹„êµ
        valid_estimates = {k: v for k, v in estimates.items() if v is not None}
        if valid_estimates:
            methods = list(valid_estimates.keys())
            values = list(valid_estimates.values())
            
            bars = axes[0,0].bar(methods, values, 
                               color=['skyblue', 'lightcoral', 'lightgreen'][:len(methods)],
                               edgecolor='navy', alpha=0.7)
            axes[0,0].set_title('Causal Effect Estimates', fontsize=14, fontweight='bold')
            axes[0,0].set_ylabel('Effect Size')
            axes[0,0].grid(True, alpha=0.3)
            
            # ê°’ ë¼ë²¨ ì¶”ê°€
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.001,
                              f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
        else:
            axes[0,0].text(0.5, 0.5, 'No valid estimates', ha='center', va='center',
                          transform=axes[0,0].transAxes, fontsize=12)
            axes[0,0].set_title('Causal Effect Estimates', fontsize=14, fontweight='bold')
        
        # 2. ì‹ ë¢°êµ¬ê°„ ì‹œê°í™” (backdoor ì¶”ì •ì´ ìˆëŠ” ê²½ìš°)
        if 'backdoor' in estimate_details and estimate_details['backdoor']:
            ci = estimate_details['backdoor']['confidence_interval']
            if ci:
                axes[0,1].errorbar(0, estimate_details['backdoor']['value'], 
                                 yerr=[[estimate_details['backdoor']['value'] - ci[0]], 
                                       [ci[1] - estimate_details['backdoor']['value']]], 
                                 fmt='o', capsize=10, capthick=2, markersize=8,
                                 color='blue', label='Backdoor Estimate')
                axes[0,1].set_xlim(-0.5, 0.5)
                axes[0,1].set_title('Confidence Interval (Backdoor)', fontsize=14, fontweight='bold')
                axes[0,1].set_ylabel('Effect Size')
                axes[0,1].legend()
                axes[0,1].grid(True, alpha=0.3)
                axes[0,1].set_xticks([])
        
        # 3. ë°˜ë°•ê²€ì‚¬ ê²°ê³¼
        if refutation_results:
            ref_methods = list(refutation_results.keys())
            ref_values = list(refutation_results.values())
            
            bars = axes[1,0].bar(ref_methods, ref_values,
                               color=['orange', 'purple'][:len(ref_methods)],
                               edgecolor='black', alpha=0.7)
            axes[1,0].set_title('Refutation Test Results', fontsize=14, fontweight='bold')
            axes[1,0].set_ylabel('New Effect Size')
            axes[1,0].tick_params(axis='x', rotation=45)
            axes[1,0].grid(True, alpha=0.3)
            
            # ê°’ ë¼ë²¨ ì¶”ê°€
            for bar, value in zip(bars, ref_values):
                height = bar.get_height()
                axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.001,
                              f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
        else:
            axes[1,0].text(0.5, 0.5, 'No refutation results', ha='center', va='center',
                          transform=axes[1,0].transAxes, fontsize=12)
            axes[1,0].set_title('Refutation Test Results', fontsize=14, fontweight='bold')
        
        # 4. ì¸ê³¼ ê·¸ë˜í”„ ì‹œê°í™”
        # ê°„ë‹¨í•œ ì¸ê³¼ ê·¸ë˜í”„ ìƒì„±
        G = nx.DiGraph()
        G.add_node(treatment, node_color='red')
        G.add_node(outcome, node_color='blue')
        for cause in common_causes:
            G.add_node(cause, node_color='gray')
            G.add_edge(cause, treatment)
            G.add_edge(cause, outcome)
        G.add_edge(treatment, outcome)
        
        pos = nx.spring_layout(G, seed=42)
        node_colors = ['red' if node == treatment else 'blue' if node == outcome else 'gray' 
                      for node in G.nodes()]
        
        nx.draw_networkx(G, pos, ax=axes[1,1], 
                        node_color=node_colors,
                        node_size=2000,
                        font_size=10,
                        arrows=True,
                        arrowsize=20,
                        with_labels=True)
        axes[1,1].set_title('Causal Graph Structure', fontsize=14, fontweight='bold')
        axes[1,1].axis('off')
        
        plt.tight_layout()
        plt.suptitle('DoWhy Framework - Causal Effect Estimation', fontsize=16, y=1.02)
        plt.show()
    
    # ê²°ë¡  ë° í•´ì„
    print(f"\nğŸ’¡ ì¸ê³¼íš¨ê³¼ í•´ì„:")
    if estimates.get('backdoor'):
        effect = estimates['backdoor']
        if effect > 0:
            print(f"   - {treatment}ì´ 1ë‹¨ìœ„ ì¦ê°€í•˜ë©´ {outcome}ì´ {effect:.4f}ë§Œí¼ ì¦ê°€")
        else:
            print(f"   - {treatment}ì´ 1ë‹¨ìœ„ ì¦ê°€í•˜ë©´ {outcome}ì´ {abs(effect):.4f}ë§Œí¼ ê°ì†Œ")
    
    if refutation_results:
        print(f"\nğŸ” ë°˜ë°•ê²€ì‚¬ ê²°ê³¼:")
        for method, value in refutation_results.items():
            if abs(value) < 0.01:  # ì„ê³„ê°’ ì„¤ì •
                print(f"   - {method}: ê°•ê±´í•¨ (íš¨ê³¼ í¬ê¸°: {value:.4f})")
            else:
                print(f"   - {method}: ì£¼ì˜ í•„ìš” (íš¨ê³¼ í¬ê¸°: {value:.4f})")
    
    print(f"\nâœ… DoWhy ì¸ê³¼íš¨ê³¼ ì¶”ì • ì™„ë£Œ!")
    
    return estimates, refutation_results, estimate_details

# ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì„± (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def build_bayesian_network(data, structure=None, visualize=True):
    """
    pgmpy í™œìš© ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ë° ëª¨ìˆ˜ í•™ìŠµ
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ë° í•™ìŠµ")
    print("=" * 60)
    
    print(f"\nğŸ”§ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ì„¤ì •:")
    print(f"   - ë°ì´í„° í¬ê¸°: {data.shape}")
    print(f"   - ë³€ìˆ˜ ìˆ˜: {len(data.columns)}ê°œ")
    
    # êµ¬ì¡° í•™ìŠµ ë˜ëŠ” ê¸°ì¡´ êµ¬ì¡° ì‚¬ìš©
    if structure is None:
        print(f"\nğŸ—ï¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° í•™ìŠµ ì¤‘ (Hill Climbing)...")
        hc = HillClimbSearch(data)
        best_model = hc.estimate(scoring_method=BicScore(data))
        edges = best_model.edges()
        print(f"   - í•™ìŠµëœ ì—£ì§€ ìˆ˜: {len(edges)}ê°œ")
    else:
        edges = structure.edges()
        print(f"\nğŸ“‹ ê¸°ì¡´ êµ¬ì¡° ì‚¬ìš©:")
        print(f"   - ì—£ì§€ ìˆ˜: {len(edges)}ê°œ")
    
    if edges:
        print(f"\nğŸ”— ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°:")
        for i, (source, target) in enumerate(edges, 1):
            print(f"   {i}. {source} â†’ {target}")
    
    # ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ìƒì„±
    print(f"\nâš™ï¸ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ ìƒì„± ì¤‘...")
    model = BayesianNetwork(edges)
    
    # ëª¨ìˆ˜ í•™ìŠµ (ê°€ìš°ì‹œì•ˆ ë¶„í¬ ê°€ì •)
    print(f"\nğŸ“Š ëª¨ìˆ˜ í•™ìŠµ ì¤‘...")
    from pgmpy.estimators import MaximumLikelihoodEstimator
    model.fit(data, estimator=MaximumLikelihoodEstimator)
    
    # ì¶”ë¡  ì—”ì§„ ì„¤ì •
    from pgmpy.inference import VariableElimination
    infer = VariableElimination(model)
    
    # ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í‰ê°€
    print(f"\nğŸ“ˆ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í‰ê°€:")
    try:
        # BIC ì ìˆ˜ ê³„ì‚°
        bic_score = BicScore(data).score(model)
        print(f"   - BIC ì ìˆ˜: {bic_score:.2f}")
        
        # AIC ì ìˆ˜ ê³„ì‚°
        from pgmpy.estimators import AicScore
        aic_score = AicScore(data).score(model)
        print(f"   - AIC ì ìˆ˜: {aic_score:.2f}")
        
        # ë„¤íŠ¸ì›Œí¬ ë³µì¡ë„
        complexity = len(edges) + len(data.columns)  # ì—£ì§€ ìˆ˜ + ë…¸ë“œ ìˆ˜
        print(f"   - ë„¤íŠ¸ì›Œí¬ ë³µì¡ë„: {complexity}")
        
    except Exception as e:
        print(f"   - ì„±ëŠ¥ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ì‹œê°í™”
    if visualize and edges:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì‹œê°í™”
        G = nx.DiGraph(edges)
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        
        # ë…¸ë“œ í¬ê¸°ë¥¼ ì—°ê²°ë„ì— ë¹„ë¡€í•˜ê²Œ ì„¤ì •
        node_sizes = [G.degree(node) * 500 + 1000 for node in G.nodes()]
        
        nx.draw_networkx(G, pos, ax=axes[0,0],
                        node_color='lightgreen',
                        node_size=node_sizes,
                        font_size=10,
                        arrows=True,
                        arrowsize=20,
                        edge_color='gray',
                        with_labels=True)
        axes[0,0].set_title('Bayesian Network Structure', fontsize=14, fontweight='bold')
        axes[0,0].axis('off')
        
        # 2. ë…¸ë“œ ì—°ê²°ë„ ë¶„í¬
        degrees = [G.degree(node) for node in G.nodes()]
        axes[0,1].hist(degrees, bins=max(1, len(set(degrees))), 
                      color='skyblue', edgecolor='black', alpha=0.7)
        axes[0,1].set_title('Node Degree Distribution', fontsize=14, fontweight='bold')
        axes[0,1].set_xlabel('Degree')
        axes[0,1].set_ylabel('Frequency')
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ë„¤íŠ¸ì›Œí¬ í†µê³„
        network_stats = {
            'Nodes': len(G.nodes()),
            'Edges': len(G.edges()),
            'Density': nx.density(G),
            'Avg Degree': np.mean(degrees) if degrees else 0,
            'Max Degree': max(degrees) if degrees else 0,
            'Min Degree': min(degrees) if degrees else 0
        }
        
        stats_df = pd.DataFrame(list(network_stats.items()), 
                               columns=['Metric', 'Value'])
        axes[1,0].axis('off')
        
        # í…Œì´ë¸” í˜•íƒœë¡œ í†µê³„ í‘œì‹œ
        table_data = []
        for metric, value in network_stats.items():
            table_data.append([metric, f'{value:.3f}' if isinstance(value, float) else str(value)])
        
        table = axes[1,0].table(cellText=table_data,
                               colLabels=['Network Metric', 'Value'],
                               cellLoc='center',
                               loc='center',
                               bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1, 2)
        axes[1,0].set_title('Network Statistics', fontsize=14, fontweight='bold', pad=20)
        
        # 4. ì¡°ê±´ë¶€ í™•ë¥  ì¶”ì • (ìƒ˜í”Œ)
        if len(data.columns) > 1:
            # ì²« ë²ˆì§¸ ë³€ìˆ˜ì˜ ë¶„í¬ ì‹œê°í™”
            first_var = data.columns[0]
            axes[1,1].hist(data[first_var], bins=30, alpha=0.7, 
                          color='lightcoral', edgecolor='black')
            axes[1,1].set_title(f'Distribution of {first_var}', fontsize=14, fontweight='bold')
            axes[1,1].set_xlabel(first_var)
            axes[1,1].set_ylabel('Frequency')
            axes[1,1].grid(True, alpha=0.3)
        else:
            axes[1,1].text(0.5, 0.5, 'Insufficient variables\nfor distribution plot', 
                          ha='center', va='center', transform=axes[1,1].transAxes, fontsize=12)
            axes[1,1].set_title('Variable Distribution', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.suptitle('Bayesian Network Analysis', fontsize=16, y=1.02)
        plt.show()
        
        # ì¶”ê°€: ì¸ê³¼ê´€ê³„ ê°•ë„ ë¶„ì„
        if len(edges) > 0:
            fig, ax = plt.subplots(figsize=(12, 6))
            
            edge_strengths = []
            edge_labels = []
            
            for source, target in edges:
                if source in data.columns and target in data.columns:
                    # ìƒê´€ê³„ìˆ˜ë¡œ ì¸ê³¼ê´€ê³„ ê°•ë„ ì¶”ì •
                    corr = data[source].corr(data[target])
                    edge_strengths.append(abs(corr))
                    edge_labels.append(f"{source}â†’{target}\n(r={corr:.3f})")
            
            if edge_strengths:
                bars = ax.bar(range(len(edge_strengths)), edge_strengths, 
                             color='lightgreen', edgecolor='darkgreen', alpha=0.7)
                ax.set_xlabel('Causal Relationships')
                ax.set_ylabel('Correlation Strength (|r|)')
                ax.set_title('Bayesian Network - Causal Relationship Strengths', 
                           fontsize=14, fontweight='bold')
                ax.set_xticks(range(len(edge_labels)))
                ax.set_xticklabels(edge_labels, rotation=45, ha='right')
                ax.grid(True, alpha=0.3)
                
                # ê°’ ë¼ë²¨ ì¶”ê°€
                for i, bar in enumerate(bars):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                plt.show()
    
    # ê²°ë¡  ë° í•´ì„
    print(f"\nğŸ’¡ ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ í•´ì„:")
    if edges:
        print(f"   - ì´ {len(edges)}ê°œì˜ ì¸ê³¼ê´€ê³„ê°€ ë°œê²¬ë¨")
        print(f"   - ë„¤íŠ¸ì›Œí¬ ë°€ë„: {nx.density(G):.3f}")
        
        # ê°€ì¥ ì—°ê²°ì´ ë§ì€ ë…¸ë“œ ì°¾ê¸°
        if G.nodes():
            max_degree_node = max(G.nodes(), key=lambda x: G.degree(x))
            print(f"   - ê°€ì¥ ì—°ê²°ì´ ë§ì€ ë³€ìˆ˜: {max_degree_node} (ì—°ê²°ë„: {G.degree(max_degree_node)})")
    else:
        print(f"   - ì¸ê³¼ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
    
    print(f"\nâœ… ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶• ì™„ë£Œ!")
    
    return model, infer, network_stats if 'network_stats' in locals() else {}



# PCMCI ì‹œê³„ì—´ ì¸ê³¼ë°œê²¬
try:
    from tigramite import data_processing as pp
    from tigramite.pcmci import PCMCI
    from tigramite.independence_tests import ParCorr
    
    def pcmci_time_series_analysis(time_series_data, tau_max=5):
        """
        PCMCI ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹œê°„ì§€ì—° ì¸ê³¼ê´€ê³„ ë¶„ì„
        """
        # ë°ì´í„° í”„ë ˆì„ êµ¬ì„±
        dataframe = pp.DataFrame(time_series_data.values, 
                               datatime=time_series_data.index,
                               var_names=time_series_data.columns.tolist())
        
        # PCMCI ì„¤ì •
        parcorr = ParCorr(significance='analytic')
        pcmci = PCMCI(dataframe=dataframe, 
                     cond_ind_test=parcorr, 
                     verbosity=1)
        
        # PCMCI ì‹¤í–‰
        results = pcmci.run_pcmci(tau_max=tau_max, 
                                 pc_alpha=0.05, 
                                 alpha_level=0.01)
        
        # ê²°ê³¼ í•´ì„
        causal_links = {}
        for j in range(len(time_series_data.columns)):
            causal_links[time_series_data.columns[j]] = []
            for i in range(len(time_series_data.columns)):
                for tau in range(tau_max + 1):
                    if results['graph'][i, j, tau] == "-->":
                        causal_links[time_series_data.columns[j]].append(
                            (time_series_data.columns[i], tau)
                        )
        
        return results, causal_links
    
except ImportError:
    print("TIGRAMITE not available. Using Granger causality as alternative.")

# Granger ì¸ê³¼ì„± ë¶„ì„ (PCMCI ëŒ€ì•ˆ) - ì‹œê°í™” ë° ê²°ë¡  í¬í•¨
from statsmodels.tsa.stattools import grangercausalitytests

def granger_causality_analysis(data, maxlag=5, visualize=True):
    """
    Granger ì¸ê³¼ì„± ê¸°ë°˜ ì‹œê°„ì§€ì—° ê´€ê³„ ë¶„ì„
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("Granger ì¸ê³¼ì„± ë¶„ì„")
    print("=" * 60)
    
    print(f"\nğŸ” Granger ì¸ê³¼ì„± ë¶„ì„ ì„¤ì •:")
    print(f"   - ìµœëŒ€ ì§€ì—° (maxlag): {maxlag}")
    print(f"   - ë¶„ì„ ë³€ìˆ˜: {len(data.columns)}ê°œ")
    print(f"   - ë°ì´í„° í¬ê¸°: {data.shape}")
    
    variables = data.columns.tolist()
    granger_matrix = pd.DataFrame(np.zeros((len(variables), len(variables))),
                                 columns=variables, index=variables)
    
    # ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
    detailed_results = {}
    significant_relationships = []
    
    print(f"\nâš™ï¸ Granger ì¸ê³¼ì„± ê²€ì • ì‹¤í–‰ ì¤‘...")
    
    for target in variables:
        detailed_results[target] = {}
        for cause in variables:
            if target != cause:
                try:
                    test_data = data[[target, cause]].dropna()
                    if len(test_data) < maxlag + 10:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ëŸ‰
                        granger_matrix.loc[cause, target] = 1.0
                        detailed_results[target][cause] = {'p_value': 1.0, 'lag': 0, 'status': 'insufficient_data'}
                        continue
                    
                    test_result = grangercausalitytests(test_data, maxlag=maxlag, verbose=False)
                    
                    # ìµœì†Œ p-valueì™€ í•´ë‹¹ lag ì°¾ê¸°
                    p_values = []
                    lag_results = {}
                    for lag in range(1, maxlag + 1):
                        if lag in test_result:
                            p_val = test_result[lag][0]['ssr_ftest'][1]
                            p_values.append(p_val)
                            lag_results[lag] = p_val
                    
                    if p_values:
                        min_p_value = min(p_values)
                        best_lag = min(lag_results, key=lag_results.get)
                        granger_matrix.loc[cause, target] = min_p_value
                        
                        detailed_results[target][cause] = {
                            'p_value': min_p_value,
                            'lag': best_lag,
                            'status': 'significant' if min_p_value < 0.05 else 'not_significant'
                        }
                        
                        if min_p_value < 0.05:
                            significant_relationships.append({
                                'cause': cause,
                                'target': target,
                                'p_value': min_p_value,
                                'lag': best_lag
                            })
                    else:
                        granger_matrix.loc[cause, target] = 1.0
                        detailed_results[target][cause] = {'p_value': 1.0, 'lag': 0, 'status': 'no_result'}
                        
                except Exception as e:
                    granger_matrix.loc[cause, target] = 1.0
                    detailed_results[target][cause] = {'p_value': 1.0, 'lag': 0, 'status': f'error: {str(e)}'}
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š Granger ì¸ê³¼ì„± ë¶„ì„ ê²°ê³¼:")
    print(f"   - ì´ ê²€ì • ìŒ: {len(variables) * (len(variables) - 1)}ê°œ")
    print(f"   - ìœ ì˜í•œ ì¸ê³¼ê´€ê³„: {len(significant_relationships)}ê°œ")
    print(f"   - ìœ ì˜ìˆ˜ì¤€: 0.05")
    
    if significant_relationships:
        print(f"\nğŸ”— ë°œê²¬ëœ ìœ ì˜í•œ ì¸ê³¼ê´€ê³„:")
        for i, rel in enumerate(significant_relationships, 1):
            print(f"   {i}. {rel['cause']} â†’ {rel['target']} (lag={rel['lag']}, p={rel['p_value']:.4f})")
    
    # ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Granger ì¸ê³¼ì„± íˆíŠ¸ë§µ
        mask = np.triu(np.ones_like(granger_matrix, dtype=bool))  # ìƒì‚¼ê° í–‰ë ¬ ë§ˆìŠ¤í¬
        sns.heatmap(granger_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                   center=0.05, ax=axes[0,0], cbar_kws={'label': 'P-value'})
        axes[0,0].set_title('Granger Causality P-values', fontsize=14, fontweight='bold')
        axes[0,0].set_xlabel('Target Variables')
        axes[0,0].set_ylabel('Cause Variables')
        
        # 2. ìœ ì˜í•œ ì¸ê³¼ê´€ê³„ë§Œ í‘œì‹œ
        significant_matrix = granger_matrix.copy()
        significant_matrix[significant_matrix >= 0.05] = np.nan  # ìœ ì˜í•˜ì§€ ì•Šì€ ê°’ì€ NaNìœ¼ë¡œ
        
        if not significant_matrix.isna().all().all():
            sns.heatmap(significant_matrix, mask=mask, annot=True, cmap='Greens', 
                       ax=axes[0,1], cbar_kws={'label': 'P-value (Significant Only)'})
            axes[0,1].set_title('Significant Granger Causality (p < 0.05)', fontsize=14, fontweight='bold')
        else:
            axes[0,1].text(0.5, 0.5, 'No significant\nGranger causality\nfound', 
                          ha='center', va='center', transform=axes[0,1].transAxes, fontsize=12)
            axes[0,1].set_title('Significant Granger Causality', fontsize=14, fontweight='bold')
        axes[0,1].set_xlabel('Target Variables')
        axes[0,1].set_ylabel('Cause Variables')
        
        # 3. ì§€ì—° ì‹œê°„ ë¶„í¬
        if significant_relationships:
            lags = [rel['lag'] for rel in significant_relationships]
            axes[1,0].hist(lags, bins=range(1, maxlag + 2), alpha=0.7, 
                          color='skyblue', edgecolor='black')
            axes[1,0].set_title('Distribution of Significant Lags', fontsize=14, fontweight='bold')
            axes[1,0].set_xlabel('Lag')
            axes[1,0].set_ylabel('Frequency')
            axes[1,0].set_xticks(range(1, maxlag + 1))
            axes[1,0].grid(True, alpha=0.3)
        else:
            axes[1,0].text(0.5, 0.5, 'No significant\nrelationships\nfor lag analysis', 
                          ha='center', va='center', transform=axes[1,0].transAxes, fontsize=12)
            axes[1,0].set_title('Distribution of Significant Lags', fontsize=14, fontweight='bold')
        
        # 4. ë³€ìˆ˜ë³„ ì¸ê³¼ì„± í†µê³„
        causality_stats = {
            'Total Tests': len(variables) * (len(variables) - 1),
            'Significant': len(significant_relationships),
            'Significance Rate': len(significant_relationships) / (len(variables) * (len(variables) - 1)) * 100,
            'Avg P-value': granger_matrix.values[granger_matrix.values < 1.0].mean() if (granger_matrix.values < 1.0).any() else 1.0
        }
        
        axes[1,1].axis('off')
        
        # í…Œì´ë¸” í˜•íƒœë¡œ í†µê³„ í‘œì‹œ
        table_data = []
        for metric, value in causality_stats.items():
            if isinstance(value, float):
                table_data.append([metric, f'{value:.3f}'])
            else:
                table_data.append([metric, str(value)])
        
        table = axes[1,1].table(cellText=table_data,
                               colLabels=['Metric', 'Value'],
                               cellLoc='center',
                               loc='center',
                               bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(12)
        table.scale(1, 2)
        axes[1,1].set_title('Granger Causality Statistics', fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.suptitle('Granger Causality Analysis', fontsize=16, y=1.02)
        plt.show()
        
        # ì¶”ê°€: ì‹œê°„ì§€ì—°ë³„ ì¸ê³¼ê´€ê³„ ê°•ë„ ë¶„ì„
        if significant_relationships:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # ì§€ì—° ì‹œê°„ë³„ë¡œ ê·¸ë£¹í™”
            lag_groups = {}
            for rel in significant_relationships:
                lag = rel['lag']
                if lag not in lag_groups:
                    lag_groups[lag] = []
                lag_groups[lag].append(rel)
            
            # ì§€ì—° ì‹œê°„ë³„ í‰ê·  p-value ê³„ì‚°
            lag_means = []
            lag_labels = []
            for lag in sorted(lag_groups.keys()):
                p_values = [rel['p_value'] for rel in lag_groups[lag]]
                lag_means.append(np.mean(p_values))
                lag_labels.append(f'Lag {lag}\n(n={len(p_values)})')
            
            bars = ax.bar(range(len(lag_means)), lag_means, 
                         color='lightcoral', edgecolor='darkred', alpha=0.7)
            ax.set_xlabel('Lag')
            ax.set_ylabel('Average P-value')
            ax.set_title('Average P-value by Lag (Significant Relationships Only)', 
                        fontsize=14, fontweight='bold')
            ax.set_xticks(range(len(lag_labels)))
            ax.set_xticklabels(lag_labels)
            ax.grid(True, alpha=0.3)
            
            # ê°’ ë¼ë²¨ ì¶”ê°€
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                       f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            plt.show()
    
    # ê²°ë¡  ë° í•´ì„
    print(f"\nğŸ’¡ Granger ì¸ê³¼ì„± í•´ì„:")
    if significant_relationships:
        print(f"   - {len(significant_relationships)}ê°œì˜ ìœ ì˜í•œ ì‹œê°„ì§€ì—° ì¸ê³¼ê´€ê³„ ë°œê²¬")
        
        # ê°€ì¥ ê°•í•œ ì¸ê³¼ê´€ê³„ ì°¾ê¸°
        strongest = min(significant_relationships, key=lambda x: x['p_value'])
        print(f"   - ê°€ì¥ ê°•í•œ ì¸ê³¼ê´€ê³„: {strongest['cause']} â†’ {strongest['target']} (p={strongest['p_value']:.4f}, lag={strongest['lag']})")
        
        # ê°€ì¥ ì¼ë°˜ì ì¸ ì§€ì—° ì‹œê°„
        if significant_relationships:
            most_common_lag = max(set(rel['lag'] for rel in significant_relationships), 
                                key=lambda x: sum(1 for rel in significant_relationships if rel['lag'] == x))
            print(f"   - ê°€ì¥ ì¼ë°˜ì ì¸ ì§€ì—° ì‹œê°„: {most_common_lag}")
    else:
        print(f"   - ìœ ì˜í•œ ì‹œê°„ì§€ì—° ì¸ê³¼ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
    
    print(f"\nâœ… Granger ì¸ê³¼ì„± ë¶„ì„ ì™„ë£Œ!")
    
    return granger_matrix, detailed_results, significant_relationships




# SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
def shap_analysis_acn_process(model, X_train, X_test, y_test=None, model_type='tree', visualize=True):
    """
    SHAPì„ ì´ìš©í•œ ACN ê³µì • ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
    ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    print("\n" + "=" * 60)
    print("SHAP ê¸°ë°˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
    print("=" * 60)
    
    print(f"\nğŸ” SHAP ë¶„ì„ ì„¤ì •:")
    print(f"   - ëª¨ë¸ íƒ€ì…: {model_type}")
    print(f"   - í›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train.shape}")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}")
    print(f"   - íŠ¹ì„± ìˆ˜: {len(X_test.columns)}ê°œ")
    
    # SHAP Explainer ì„¤ì •
    print(f"\nâš™ï¸ SHAP Explainer ì„¤ì • ì¤‘...")
    try:
        if model_type == 'tree':
            explainer = shap.TreeExplainer(model)
        elif model_type == 'linear':
            explainer = shap.LinearExplainer(model, X_train)
        else:
            explainer = shap.Explainer(model, X_train)
        print(f"   âœ… {model_type} Explainer ì„¤ì • ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ Explainer ì„¤ì • ì‹¤íŒ¨: {str(e)}")
        return None, None, None
    
    # SHAP values ê³„ì‚°
    print(f"\nğŸ“Š SHAP values ê³„ì‚° ì¤‘...")
    try:
        shap_values = explainer.shap_values(X_test)
        print(f"   âœ… SHAP values ê³„ì‚° ì™„ë£Œ")
    except Exception as e:
        print(f"   âŒ SHAP values ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
        return explainer, None, None
    
    # ì¤‘ìš”ë„ ìˆœìœ„ ê³„ì‚°
    if len(shap_values.shape) > 1:
        mean_shap = np.abs(shap_values).mean(0)
    else:
        mean_shap = np.abs(shap_values).mean()
    
    feature_importance = pd.DataFrame({
        'feature': X_test.columns,
        'mean_shap': mean_shap,
        'std_shap': np.abs(shap_values).std(0) if len(shap_values.shape) > 1 else np.abs(shap_values).std()
    }).sort_values('mean_shap', ascending=False)
    
    # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    if y_test is not None:
        try:
            y_pred = model.predict(X_test)
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            print(f"\nğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥:")
            print(f"   - RÂ² Score: {r2:.4f}")
            print(f"   - MSE: {mse:.4f}")
        except Exception as e:
            print(f"   - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“‹ SHAP ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼:")
    print(f"   - ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['mean_shap']:.4f})")
    print(f"   - ê°€ì¥ ëœ ì¤‘ìš”í•œ íŠ¹ì„±: {feature_importance.iloc[-1]['feature']} ({feature_importance.iloc[-1]['mean_shap']:.4f})")
    print(f"   - ì¤‘ìš”ë„ ë²”ìœ„: {feature_importance['mean_shap'].min():.4f} ~ {feature_importance['mean_shap'].max():.4f}")
    
    # ìƒìœ„ 3ê°œ íŠ¹ì„± ì¶œë ¥
    print(f"\nğŸ† ìƒìœ„ 3ê°œ ì¤‘ìš” íŠ¹ì„±:")
    for i in range(min(3, len(feature_importance))):
        feature = feature_importance.iloc[i]
        print(f"   {i+1}. {feature['feature']}: {feature['mean_shap']:.4f} Â± {feature['std_shap']:.4f}")
    
    # ì‹œê°í™”
    if visualize:
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. íŠ¹ì„± ì¤‘ìš”ë„ ë§‰ëŒ€ ê·¸ë˜í”„
        top_features = feature_importance.head(10)  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
        bars = axes[0,0].barh(range(len(top_features)), top_features['mean_shap'], 
                             color='skyblue', edgecolor='navy', alpha=0.7)
        axes[0,0].set_yticks(range(len(top_features)))
        axes[0,0].set_yticklabels(top_features['feature'])
        axes[0,0].set_xlabel('Mean |SHAP value|')
        axes[0,0].set_title('Top 10 Feature Importance (SHAP)', fontsize=14, fontweight='bold')
        axes[0,0].grid(True, alpha=0.3)
        
        # ê°’ ë¼ë²¨ ì¶”ê°€
        for i, bar in enumerate(bars):
            width = bar.get_width()
            axes[0,0].text(width + 0.001, bar.get_y() + bar.get_height()/2.,
                          f'{width:.3f}', ha='left', va='center', fontweight='bold')
        
        # 2. SHAP Summary Plot (ìƒìœ„ 10ê°œ íŠ¹ì„±)
        if len(shap_values.shape) > 1:
            # ìƒ˜í”Œ ìˆ˜ë¥¼ ì œí•œí•˜ì—¬ ì‹œê°í™” ì„±ëŠ¥ í–¥ìƒ
            sample_size = min(100, len(X_test))
            sample_indices = np.random.choice(len(X_test), sample_size, replace=False)
            
            shap.summary_plot(shap_values[sample_indices], X_test.iloc[sample_indices], 
                             max_display=10, show=False, ax=axes[0,1])
            axes[0,1].set_title('SHAP Summary Plot (Top 10 Features)', fontsize=14, fontweight='bold')
        else:
            axes[0,1].text(0.5, 0.5, 'SHAP Summary Plot\nnot available for this model type', 
                          ha='center', va='center', transform=axes[0,1].transAxes, fontsize=12)
            axes[0,1].set_title('SHAP Summary Plot', fontsize=14, fontweight='bold')
        
        # 3. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„í¬
        axes[1,0].hist(feature_importance['mean_shap'], bins=20, alpha=0.7, 
                      color='lightcoral', edgecolor='black')
        axes[1,0].set_xlabel('Mean |SHAP value|')
        axes[1,0].set_ylabel('Frequency')
        axes[1,0].set_title('Distribution of Feature Importance', fontsize=14, fontweight='bold')
        axes[1,0].grid(True, alpha=0.3)
        
        # 4. íŠ¹ì„±ë³„ SHAP ê°’ ë¶„ì‚°
        axes[1,1].scatter(feature_importance['mean_shap'], feature_importance['std_shap'], 
                         alpha=0.7, s=100, color='green', edgecolor='darkgreen')
        axes[1,1].set_xlabel('Mean |SHAP value|')
        axes[1,1].set_ylabel('Std |SHAP value|')
        axes[1,1].set_title('Feature Importance vs Variability', fontsize=14, fontweight='bold')
        axes[1,1].grid(True, alpha=0.3)
        
        # íŠ¹ì„±ëª… ë¼ë²¨ ì¶”ê°€
        for i, feature in enumerate(feature_importance['feature']):
            axes[1,1].annotate(feature, 
                              (feature_importance.iloc[i]['mean_shap'], 
                               feature_importance.iloc[i]['std_shap']),
                              fontsize=8, alpha=0.7)
        
        plt.tight_layout()
        plt.suptitle('SHAP Feature Importance Analysis', fontsize=16, y=1.02)
        plt.show()
        
        # ì¶”ê°€: Waterfall Plot (ì²« ë²ˆì§¸ ìƒ˜í”Œ)
        if len(shap_values.shape) > 1 and len(shap_values) > 0:
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ SHAP ê°’ìœ¼ë¡œ waterfall plot ìƒì„±
            sample_shap = shap_values[0]
            base_value = explainer.expected_value if hasattr(explainer, 'expected_value') else 0
            
            # íŠ¹ì„±ë³„ ê¸°ì—¬ë„ ê³„ì‚°
            contributions = []
            feature_names = []
            for i, feature in enumerate(X_test.columns):
                contributions.append(sample_shap[i])
                feature_names.append(feature)
            
            # ê¸°ì—¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_indices = np.argsort(np.abs(contributions))[::-1]
            sorted_contributions = [contributions[i] for i in sorted_indices]
            sorted_features = [feature_names[i] for i in sorted_indices]
            
            # Waterfall plot ìƒì„±
            cumulative = base_value
            positions = range(len(sorted_contributions))
            
            for i, (contrib, feature) in enumerate(zip(sorted_contributions, sorted_features)):
                color = 'green' if contrib > 0 else 'red'
                ax.bar(i, contrib, bottom=cumulative, color=color, alpha=0.7, 
                      edgecolor='black', linewidth=0.5)
                cumulative += contrib
            
            ax.axhline(y=base_value, color='blue', linestyle='--', alpha=0.7, label=f'Base Value: {base_value:.3f}')
            ax.axhline(y=cumulative, color='purple', linestyle='--', alpha=0.7, label=f'Final Prediction: {cumulative:.3f}')
            
            ax.set_xticks(positions)
            ax.set_xticklabels(sorted_features, rotation=45, ha='right')
            ax.set_ylabel('SHAP Value')
            ax.set_title('SHAP Waterfall Plot (First Sample)', fontsize=14, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
    
    # ê²°ë¡  ë° í•´ì„
    print(f"\nğŸ’¡ SHAP ë¶„ì„ í•´ì„:")
    print(f"   - ì´ {len(feature_importance)}ê°œ íŠ¹ì„± ë¶„ì„")
    print(f"   - ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” íŠ¹ì„±: {feature_importance.iloc[0]['feature']}")
    print(f"   - ì¤‘ìš”ë„ ì°¨ì´: {feature_importance.iloc[0]['mean_shap'] / feature_importance.iloc[-1]['mean_shap']:.1f}ë°°")
    
    # íŠ¹ì„± ê·¸ë£¹ë³„ ë¶„ì„
    high_importance = feature_importance[feature_importance['mean_shap'] > feature_importance['mean_shap'].quantile(0.75)]
    medium_importance = feature_importance[(feature_importance['mean_shap'] > feature_importance['mean_shap'].quantile(0.25)) & 
                                         (feature_importance['mean_shap'] <= feature_importance['mean_shap'].quantile(0.75))]
    low_importance = feature_importance[feature_importance['mean_shap'] <= feature_importance['mean_shap'].quantile(0.25)]
    
    print(f"\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë£¹ë³„ ë¶„ì„:")
    print(f"   - ê³ ì¤‘ìš”ë„ íŠ¹ì„±: {len(high_importance)}ê°œ")
    print(f"   - ì¤‘ì¤‘ìš”ë„ íŠ¹ì„±: {len(medium_importance)}ê°œ")
    print(f"   - ì €ì¤‘ìš”ë„ íŠ¹ì„±: {len(low_importance)}ê°œ")
    
    print(f"\nâœ… SHAP ë¶„ì„ ì™„ë£Œ!")
    
    return explainer, shap_values, feature_importance

# í†µí•© ì‹œê°í™” í•¨ìˆ˜ (ì¤‘ë³µ ì œê±° ë° ê°œì„ )
def create_comprehensive_visualization(dag, shap_importance, granger_matrix, 
                                     bayesian_stats=None, dowhy_results=None):
    """
    ëª¨ë“  ì¸ê³¼ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ì‹œê°í™”
    """
    print("\n" + "=" * 60)
    print("ì¢…í•© ì¸ê³¼ë¶„ì„ ê²°ê³¼ ì‹œê°í™”")
    print("=" * 60)
    
    fig, axes = plt.subplots(3, 2, figsize=(18, 20))
    
    # 1. DAG ì‹œê°í™”
    if dag and len(dag.edges()) > 0:
        pos = nx.spring_layout(dag, k=3, iterations=50, seed=42)
        node_sizes = [dag.degree(node) * 500 + 1000 for node in dag.nodes()]
        nx.draw_networkx(dag, pos, ax=axes[0,0], 
                        node_color='lightblue', 
                        node_size=node_sizes,
                        font_size=10, 
                        arrows=True,
                        arrowsize=20,
                        edge_color='gray')
        axes[0,0].set_title('Causal DAG Structure (PC Algorithm)', fontsize=14, fontweight='bold')
    else:
        axes[0,0].text(0.5, 0.5, 'No causal structure\nfound', ha='center', va='center',
                      transform=axes[0,0].transAxes, fontsize=12)
        axes[0,0].set_title('Causal DAG Structure', fontsize=14, fontweight='bold')
    axes[0,0].axis('off')
    
    # 2. SHAP ì¤‘ìš”ë„
    if shap_importance is not None and len(shap_importance) > 0:
        top_features = shap_importance.head(8)  # ìƒìœ„ 8ê°œë§Œ í‘œì‹œ
        bars = axes[0,1].barh(range(len(top_features)), top_features['mean_shap'],
                             color='skyblue', edgecolor='navy', alpha=0.7)
        axes[0,1].set_yticks(range(len(top_features)))
        axes[0,1].set_yticklabels(top_features['feature'])
        axes[0,1].set_xlabel('Mean |SHAP value|')
        axes[0,1].set_title('SHAP Feature Importance', fontsize=14, fontweight='bold')
        axes[0,1].grid(True, alpha=0.3)
        
        # ê°’ ë¼ë²¨ ì¶”ê°€
        for i, bar in enumerate(bars):
            width = bar.get_width()
            axes[0,1].text(width + 0.001, bar.get_y() + bar.get_height()/2.,
                          f'{width:.3f}', ha='left', va='center', fontweight='bold')
    else:
        axes[0,1].text(0.5, 0.5, 'SHAP analysis\nnot available', ha='center', va='center',
                      transform=axes[0,1].transAxes, fontsize=12)
        axes[0,1].set_title('SHAP Feature Importance', fontsize=14, fontweight='bold')
    
    # 3. Granger ì¸ê³¼ì„± íˆíŠ¸ë§µ
    if granger_matrix is not None:
        mask = np.triu(np.ones_like(granger_matrix, dtype=bool))
        sns.heatmap(granger_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                   center=0.05, ax=axes[1,0], cbar_kws={'label': 'P-value'})
        axes[1,0].set_title('Granger Causality P-values', fontsize=14, fontweight='bold')
        axes[1,0].set_xlabel('Target Variables')
        axes[1,0].set_ylabel('Cause Variables')
    else:
        axes[1,0].text(0.5, 0.5, 'Granger causality\nanalysis not available', ha='center', va='center',
                      transform=axes[1,0].transAxes, fontsize=12)
        axes[1,0].set_title('Granger Causality', fontsize=14, fontweight='bold')
    
    # 4. ê²°í•©ëœ ì¤‘ìš”ë„ ë¶„ì„
    if dag and shap_importance is not None:
        try:
            dag_centrality = pd.Series(nx.degree_centrality(dag))
            combined_df = pd.DataFrame({
                'dag_centrality': dag_centrality,
                'shap_importance': shap_importance.set_index('feature')['mean_shap']
            }).fillna(0)
            
            if len(combined_df) > 0:
                scatter = axes[1,1].scatter(combined_df['dag_centrality'], 
                                          combined_df['shap_importance'],
                                          s=100, alpha=0.7, c='green', edgecolor='darkgreen')
                axes[1,1].set_xlabel('DAG Centrality')
                axes[1,1].set_ylabel('SHAP Importance')
                axes[1,1].set_title('Causal Structure vs. Feature Importance', fontsize=14, fontweight='bold')
                axes[1,1].grid(True, alpha=0.3)
                
                # ê° ì ì— ë³€ìˆ˜ëª… ë¼ë²¨ë§
                for i, txt in enumerate(combined_df.index):
                    axes[1,1].annotate(txt, 
                                      (combined_df['dag_centrality'].iloc[i],
                                       combined_df['shap_importance'].iloc[i]),
                                      fontsize=8, alpha=0.7)
            else:
                axes[1,1].text(0.5, 0.5, 'No data for\ncombined analysis', ha='center', va='center',
                              transform=axes[1,1].transAxes, fontsize=12)
                axes[1,1].set_title('Combined Analysis', fontsize=14, fontweight='bold')
        except Exception as e:
            axes[1,1].text(0.5, 0.5, f'Combined analysis\nfailed: {str(e)[:30]}...', ha='center', va='center',
                          transform=axes[1,1].transAxes, fontsize=12)
            axes[1,1].set_title('Combined Analysis', fontsize=14, fontweight='bold')
    else:
        axes[1,1].text(0.5, 0.5, 'Insufficient data for\ncombined analysis', ha='center', va='center',
                      transform=axes[1,1].transAxes, fontsize=12)
        axes[1,1].set_title('Combined Analysis', fontsize=14, fontweight='bold')
    
    # 5. ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ í†µê³„
    if bayesian_stats:
        stats_data = []
        for metric, value in bayesian_stats.items():
            stats_data.append([metric, f'{value:.3f}' if isinstance(value, float) else str(value)])
        
        axes[2,0].axis('off')
        table = axes[2,0].table(cellText=stats_data,
                               colLabels=['Bayesian Network Metric', 'Value'],
                               cellLoc='center',
                               loc='center',
                               bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 1.5)
        axes[2,0].set_title('Bayesian Network Statistics', fontsize=14, fontweight='bold', pad=20)
    else:
        axes[2,0].text(0.5, 0.5, 'Bayesian network\nstatistics not available', ha='center', va='center',
                      transform=axes[2,0].transAxes, fontsize=12)
        axes[2,0].set_title('Bayesian Network Statistics', fontsize=14, fontweight='bold')
    
    # 6. DoWhy ê²°ê³¼ ìš”ì•½
    if dowhy_results:
        estimates = dowhy_results.get('estimates', {})
        valid_estimates = {k: v for k, v in estimates.items() if v is not None}
        
        if valid_estimates:
            methods = list(valid_estimates.keys())
            values = list(valid_estimates.values())
            
            bars = axes[2,1].bar(methods, values, 
                               color=['skyblue', 'lightcoral', 'lightgreen'][:len(methods)],
                               edgecolor='navy', alpha=0.7)
            axes[2,1].set_title('DoWhy Causal Effect Estimates', fontsize=14, fontweight='bold')
            axes[2,1].set_ylabel('Effect Size')
            axes[2,1].grid(True, alpha=0.3)
            
            # ê°’ ë¼ë²¨ ì¶”ê°€
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[2,1].text(bar.get_x() + bar.get_width()/2., height + 0.001,
                              f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
        else:
            axes[2,1].text(0.5, 0.5, 'No valid DoWhy\nestimates available', ha='center', va='center',
                          transform=axes[2,1].transAxes, fontsize=12)
            axes[2,1].set_title('DoWhy Results', fontsize=14, fontweight='bold')
    else:
        axes[2,1].text(0.5, 0.5, 'DoWhy analysis\nnot available', ha='center', va='center',
                      transform=axes[2,1].transAxes, fontsize=12)
        axes[2,1].set_title('DoWhy Results', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.suptitle('Comprehensive Causal Analysis Results', fontsize=18, y=1.02)
    plt.show()
    
    print("âœ… ì¢…í•© ì‹œê°í™” ì™„ë£Œ!")
    
    return fig




class ACNCausalAnalyzer:
    """
    ACN ê³µì • ì¸ê³¼ë¶„ì„ì„ ìœ„í•œ í†µí•© í´ë˜ìŠ¤ (ê°œì„ ëœ ë²„ì „)
    ëª¨ë“  ë¶„ì„ ê³¼ì •ì— ì‹œê°í™”ì™€ ê²°ë¡  ì¶œë ¥ í¬í•¨
    """
    
    def __init__(self, data, visualize=True):
        self.raw_data = data
        self.processed_data = None
        self.causal_dag = None
        self.bayesian_model = None
        self.shap_results = None
        self.time_series_results = None
        self.dowhy_results = None
        self.bayesian_stats = None
        self.visualize = visualize
        
    def preprocess_data(self, vif_threshold=5):
        """ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        self.processed_data, self.scaler, self.outlier_info = preprocess_acn_data(
            self.raw_data, visualize=self.visualize
        )
        
        # VIF ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
        features_to_keep, self.removed_features, self.final_vif = handle_multicollinearity(
            self.processed_data.select_dtypes(include=[np.number]), 
            threshold=vif_threshold, visualize=self.visualize
        )
        
        # acn_yieldê°€ ìˆë‹¤ë©´ í¬í•¨
        if 'acn_yield' in self.processed_data.columns:
            features_to_keep = features_to_keep + ['acn_yield'] if 'acn_yield' not in features_to_keep else features_to_keep
        
        self.processed_data = self.processed_data[features_to_keep]
        
    def discover_causal_structure(self, method='pc', alpha=0.05):
        """ì¸ê³¼êµ¬ì¡° ë°œê²¬ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        if method == 'pc':
            self.causal_dag, self.pc_result, self.centrality_measures, self.edges_found = discover_causal_structure_pc(
                self.processed_data, alpha=alpha, visualize=self.visualize
            )
        elif method == 'ges':
            # GES êµ¬í˜„ (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
            print("GES method not yet implemented. Using PC algorithm instead.")
            self.causal_dag, self.pc_result, self.centrality_measures, self.edges_found = discover_causal_structure_pc(
                self.processed_data, alpha=alpha, visualize=self.visualize
            )
        
    def analyze_time_series_causality(self, time_col=None, tau_max=5):
        """ì‹œê³„ì—´ ì¸ê³¼ê´€ê³„ ë¶„ì„ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        if time_col and time_col in self.processed_data.columns:
            ts_data = self.processed_data.set_index(time_col)
        else:
            ts_data = self.processed_data
        
        # Granger causality analysis
        self.time_series_results, self.granger_details, self.significant_relationships = granger_causality_analysis(
            ts_data, maxlag=tau_max, visualize=self.visualize
        )
        
    def fit_bayesian_network(self):
        """ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        self.bayesian_model, self.inference_engine, self.bayesian_stats = build_bayesian_network(
            self.processed_data, structure=self.causal_dag, visualize=self.visualize
        )
        
    def explain_with_shap(self, target_var='acn_yield', model_type='rf'):
        """SHAP ê¸°ë°˜ ì„¤ëª… (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        # ê°„ë‹¨í•œ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (SHAP ë¶„ì„ìš©)
        X = self.processed_data.drop(columns=[target_var])
        y = self.processed_data[target_var]
        
        if model_type == 'rf':
            model = RandomForestRegressor(n_estimators=100, random_state=42)
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        
        model.fit(X_train, y_train)
        
        # SHAP ë¶„ì„
        explainer, shap_values, importance = shap_analysis_acn_process(
            model, X_train, X_test, y_test, model_type='tree', visualize=self.visualize
        )
        
        self.shap_results = {
            'explainer': explainer,
            'shap_values': shap_values,
            'importance': importance,
            'model': model,
            'X_test': X_test,
            'y_test': y_test
        }
        
    def estimate_causal_effects(self, treatment='temperature', outcome='acn_yield'):
        """DoWhy ê¸°ë°˜ ì¸ê³¼íš¨ê³¼ ì¶”ì • (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)"""
        # ê³µí†µ ì›ì¸ ì„¤ì • (treatmentì™€ outcomeì„ ì œì™¸í•œ ë³€ìˆ˜ë“¤)
        common_causes = [col for col in self.processed_data.columns 
                        if col not in [treatment, outcome]]
        
        if len(common_causes) == 0:
            print("âš ï¸ ê³µí†µ ì›ì¸ì´ ì—†ì–´ DoWhy ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        estimates, refutation_results, estimate_details = estimate_causal_effect_dowhy(
            self.processed_data, treatment, outcome, common_causes, visualize=self.visualize
        )
        
        self.dowhy_results = {
            'estimates': estimates,
            'refutation_results': refutation_results,
            'estimate_details': estimate_details,
            'treatment': treatment,
            'outcome': outcome,
            'common_causes': common_causes
        }
        
    def generate_optimization_recommendations(self):
        """ìµœì í™” ê¶Œê³ ì‚¬í•­ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        print("\n" + "=" * 60)
        print("ìµœì í™” ê¶Œê³ ì‚¬í•­ ìƒì„±")
        print("=" * 60)
        
        recommendations = {}
        
        # SHAP ì¤‘ìš”ë„ ê¸°ë°˜ ê¶Œê³ 
        if self.shap_results and self.shap_results['importance'] is not None:
            top_features = self.shap_results['importance'].head(3)
            recommendations['top_influential_vars'] = top_features['feature'].tolist()
            print(f"\nğŸ¯ SHAP ê¸°ë°˜ ê¶Œê³ :")
            print(f"   - ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë³€ìˆ˜: {top_features.iloc[0]['feature']}")
            print(f"   - ìƒìœ„ 3ê°œ ë³€ìˆ˜: {', '.join(top_features['feature'].tolist())}")
        
        # ì¸ê³¼êµ¬ì¡° ê¸°ë°˜ ê¶Œê³ 
        if self.causal_dag and len(self.causal_dag.edges()) > 0:
            centrality = nx.degree_centrality(self.causal_dag)
            most_central = max(centrality, key=centrality.get)
            recommendations['most_connected_var'] = most_central
            print(f"\nğŸ”— ì¸ê³¼êµ¬ì¡° ê¸°ë°˜ ê¶Œê³ :")
            print(f"   - ê°€ì¥ ì—°ê²°ì´ ë§ì€ ë³€ìˆ˜: {most_central} (ì¤‘ì‹¬ì„±: {centrality[most_central]:.3f})")
        
        # ì‹œê³„ì—´ ë¶„ì„ ê¸°ë°˜ ê¶Œê³ 
        if hasattr(self, 'significant_relationships') and self.significant_relationships:
            # ACN yieldì— ê°€ì¥ ê°•í•œ Granger cause ì°¾ê¸°
            yield_causes = [rel for rel in self.significant_relationships 
                          if rel['target'] == 'acn_yield']
            if yield_causes:
                strongest_cause = min(yield_causes, key=lambda x: x['p_value'])
                recommendations['strongest_temporal_cause'] = strongest_cause['cause']
                print(f"\nâ° ì‹œê³„ì—´ ë¶„ì„ ê¸°ë°˜ ê¶Œê³ :")
                print(f"   - ACN yieldì˜ ê°€ì¥ ê°•í•œ ì‹œê°„ì§€ì—° ì›ì¸: {strongest_cause['cause']}")
                print(f"   - ì§€ì—° ì‹œê°„: {strongest_cause['lag']}, p-value: {strongest_cause['p_value']:.4f}")
        
        # DoWhy ê²°ê³¼ ê¸°ë°˜ ê¶Œê³ 
        if self.dowhy_results and self.dowhy_results['estimates']:
            backdoor_effect = self.dowhy_results['estimates'].get('backdoor')
            if backdoor_effect is not None:
                treatment = self.dowhy_results['treatment']
                outcome = self.dowhy_results['outcome']
                recommendations['causal_effect'] = {
                    'treatment': treatment,
                    'outcome': outcome,
                    'effect_size': backdoor_effect
                }
                print(f"\nğŸ“Š ì¸ê³¼íš¨ê³¼ ê¸°ë°˜ ê¶Œê³ :")
                print(f"   - {treatment} â†’ {outcome} ì¸ê³¼íš¨ê³¼: {backdoor_effect:.4f}")
        
        # ì¢…í•© ê¶Œê³ ì‚¬í•­
        print(f"\nğŸ’¡ ì¢…í•© ìµœì í™” ê¶Œê³ ì‚¬í•­:")
        if 'top_influential_vars' in recommendations:
            print(f"   1. ì£¼ìš” ì œì–´ ë³€ìˆ˜: {recommendations['top_influential_vars'][0]} ì§‘ì¤‘ ê´€ë¦¬")
        if 'most_connected_var' in recommendations:
            print(f"   2. ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ ë³€ìˆ˜: {recommendations['most_connected_var']} ëª¨ë‹ˆí„°ë§ ê°•í™”")
        if 'strongest_temporal_cause' in recommendations:
            print(f"   3. ì‹œê°„ì§€ì—° ê³ ë ¤: {recommendations['strongest_temporal_cause']} ì¡°ê¸° ëŒ€ì‘")
        
        return recommendations
        
    def run_full_analysis(self, treatment='temperature', outcome='acn_yield'):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „)"""
        print("ğŸš€ ACN ê³µì • ì¢…í•© ì¸ê³¼ë¶„ì„ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1. ì „ì²˜ë¦¬
            print("\nğŸ“‹ Step 1: ë°ì´í„° ì „ì²˜ë¦¬")
            self.preprocess_data()
            
            # 2. ì¸ê³¼êµ¬ì¡° ë°œê²¬
            print("\nğŸ” Step 2: ì¸ê³¼êµ¬ì¡° ë°œê²¬")
            self.discover_causal_structure(method='pc')
            
            # 3. ì‹œê³„ì—´ ë¶„ì„
            print("\nâ° Step 3: ì‹œê³„ì—´ ì¸ê³¼ê´€ê³„ ë¶„ì„")
            self.analyze_time_series_causality()
            
            # 4. ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬
            print("\nğŸ§  Step 4: ë² ì´ì§€ì•ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•")
            self.fit_bayesian_network()
            
            # 5. SHAP ë¶„ì„
            print("\nğŸ“Š Step 5: SHAP íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")
            self.explain_with_shap(target_var=outcome)
            
            # 6. DoWhy ì¸ê³¼íš¨ê³¼ ì¶”ì •
            print("\nğŸ¯ Step 6: DoWhy ì¸ê³¼íš¨ê³¼ ì¶”ì •")
            self.estimate_causal_effects(treatment=treatment, outcome=outcome)
            
            # 7. ê¶Œê³ ì‚¬í•­ ìƒì„±
            print("\nğŸ’¡ Step 7: ìµœì í™” ê¶Œê³ ì‚¬í•­ ìƒì„±")
            recommendations = self.generate_optimization_recommendations()
            
            # 8. ì¢…í•© ì‹œê°í™”
            if self.visualize:
                print("\nğŸ“ˆ Step 8: ì¢…í•© ê²°ê³¼ ì‹œê°í™”")
                create_comprehensive_visualization(
                    self.causal_dag,
                    self.shap_results['importance'] if self.shap_results else None,
                    self.time_series_results,
                    self.bayesian_stats,
                    self.dowhy_results
                )
            
            print("\n" + "=" * 80)
            print("ğŸ‰ ACN ê³µì • ì¢…í•© ì¸ê³¼ë¶„ì„ ì™„ë£Œ!")
            print("=" * 80)
            
            return recommendations
            
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print("ë¶€ë¶„ì ì¸ ê²°ê³¼ë¼ë„ í™•ì¸í•´ë³´ì„¸ìš”.")
            return {}

# ì‚¬ìš© ì˜ˆì œ (ê°œì„ ëœ ë²„ì „)
def main():
    """
    ACN ê³µì • ë°ì´í„° ë¶„ì„ ì‹¤í–‰ ì˜ˆì œ (ì‹œê°í™” ë° ê²°ë¡  í¬í•¨)
    """
    print("ğŸ”¬ ACN ê³µì • ì¸ê³¼ì¶”ë¡  ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ì‚¬ìš©ì‹œ ì‹¤ê³µì • ë°ì´í„° ë¡œë“œ)
    np.random.seed(42)
    n_samples = 1000
    
    print(f"\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘... (n={n_samples})")
    
    sample_data = pd.DataFrame({
        'temperature': np.random.normal(450, 10, n_samples),
        'pressure': np.random.normal(150, 20, n_samples), 
        'nh3_c3h6_ratio': np.random.normal(1.15, 0.05, n_samples),
        'o2_c3h6_ratio': np.random.normal(1.75, 0.1, n_samples),
        'ghsv': np.random.normal(1000, 100, n_samples),
        'catalyst_age': np.random.uniform(0, 365, n_samples),
    })
    
    # ACN yield ê³„ì‚° (temperatureê°€ ì£¼ìš” ì˜í–¥ì¸ìë¡œ ì„¤ì •)
    sample_data['acn_yield'] = (
        0.85 + 
        0.02 * (sample_data['temperature'] - 450) / 10 +
        0.01 * (sample_data['nh3_c3h6_ratio'] - 1.15) / 0.05 +
        -0.005 * sample_data['catalyst_age'] / 365 +
        np.random.normal(0, 0.02, n_samples)
    )
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"   - ë°ì´í„° í¬ê¸°: {sample_data.shape}")
    print(f"   - ë³€ìˆ˜: {list(sample_data.columns)}")
    
    # ë¶„ì„ ì‹¤í–‰
    print(f"\nğŸš€ ì¸ê³¼ë¶„ì„ ì‹œì‘...")
    analyzer = ACNCausalAnalyzer(sample_data, visualize=True)
    recommendations = analyzer.run_full_analysis(
        treatment='temperature', 
        outcome='acn_yield'
    )
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n" + "=" * 80)
    print("ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    
    if recommendations:
        print(f"\nğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­:")
        for key, value in recommendations.items():
            if isinstance(value, list):
                print(f"   - {key}: {', '.join(map(str, value))}")
            elif isinstance(value, dict):
                print(f"   - {key}: {value}")
            else:
                print(f"   - {key}: {value}")
    
    print(f"\nğŸ’¡ í™œìš© ë°©ì•ˆ:")
    print(f"   1. ê³µì • ìµœì í™”: ì£¼ìš” ì˜í–¥ ë³€ìˆ˜ ì§‘ì¤‘ ê´€ë¦¬")
    print(f"   2. í’ˆì§ˆ í–¥ìƒ: ì¸ê³¼ê´€ê³„ ê¸°ë°˜ ì œì–´ ì „ëµ ìˆ˜ë¦½")
    print(f"   3. ì˜ˆì¸¡ ëª¨ë¸: ë°œê²¬ëœ ì¸ê³¼êµ¬ì¡°ë¥¼ í™œìš©í•œ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•")
    print(f"   4. ëª¨ë‹ˆí„°ë§: ì‹œê°„ì§€ì—° ê´€ê³„ë¥¼ ê³ ë ¤í•œ ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ")
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ê°€ ì‹œê°í™”ì™€ í•¨ê»˜ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
