import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.interpolate import interp1d
from scipy.stats import pearsonr, spearmanr
from scipy.fft import fft, fftfreq
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.decomposition import PCA
from dtaidistance import dtw
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = (14, 8)
sns.set_style("whitegrid")

# ============================================================================
# 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´ í•„ìš”)
# ============================================================================
def generate_sample_data(n_batches=50):
    """
    ìƒ˜í”Œ ACN ì •ì œ ë°°ì¹˜ ë°ì´í„° ìƒì„±
    ê° ë°°ì¹˜: phase1, phase2, phase3ì˜ ì‹œê³„ì—´ ë°ì´í„°
    """
    data = []
    
    for batch_id in range(n_batches):
        # ê° phaseì˜ ê¸¸ì´ (ê°€ë³€)
        len_p1 = np.random.randint(80, 150)
        len_p2 = np.random.randint(100, 200)
        len_p3 = np.random.randint(60, 120)
        
        # ê¸°ë³¸ ê¶¤ì ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        noise_scale = np.random.uniform(0.3, 1.5)
        golden_factor = np.random.uniform(0.6, 1.4)  # ê³ ìˆ˜ìœ¨ ê·¼ì ‘ë„
        
        # Phase 1: ìƒìŠ¹ ì¶”ì„¸
        p1 = 20 + 30 * np.linspace(0, 1, len_p1) + np.random.normal(0, noise_scale, len_p1)
        
        # Phase 2: ì˜¤ì‹¤ë ˆì´ì…˜ í¬í•¨ (golden cycleì€ ì‘ì€ ì˜¤ì‹¤ë ˆì´ì…˜)
        t2 = np.linspace(0, 4*np.pi, len_p2)
        p2 = 50 + 15*np.sin(t2) * golden_factor + np.random.normal(0, noise_scale, len_p2)
        
        # Phase 3: í•˜ê°• ì¶”ì„¸
        p3 = 65 - 20 * np.linspace(0, 1, len_p3) + np.random.normal(0, noise_scale, len_p3)
        
        # ìˆ˜ìœ¨ ê³„ì‚° (ì˜¤ì‹¤ë ˆì´ì…˜ ì•ˆì •ì„±, ê¸°ìš¸ê¸° ë“±ì— ë”°ë¼)
        osc_quality = 1.0 / (1.0 + np.std(np.diff(p2)))  # ì‘ì€ ì˜¤ì‹¤ë ˆì´ì…˜ì´ ì¢‹ìŒ
        trend_quality = np.abs(np.mean(np.diff(p1))) / 10.0  # ì ì ˆí•œ ìƒìŠ¹ ê¸°ìš¸ê¸°
        yield_val = 70 + 20 * osc_quality + 10 * trend_quality + np.random.normal(0, 3)
        yield_val = np.clip(yield_val, 50, 95)
        
        batch_data = {
            'batch_id': f'B{batch_id:03d}',
            'phase1': p1.tolist(),
            'phase2': p2.tolist(),
            'phase3': p3.tolist(),
            'yield': yield_val
        }
        data.append(batch_data)
    
    return data

# ============================================================================
# 2. ì‹œê°„ ì •ê·œí™” ë° ë³´ê°„
# ============================================================================
def normalize_timeseries(phase_data, target_length=200):
    """
    ê°€ë³€ ê¸¸ì´ì˜ phaseë¥¼ ê³ ì • ê¸¸ì´ë¡œ ì •ê·œí™”
    """
    original_length = len(phase_data)
    x_old = np.linspace(0, 1, original_length)
    x_new = np.linspace(0, 1, target_length)
    
    f = interp1d(x_old, phase_data, kind='cubic', fill_value='extrapolate')
    normalized = f(x_new)
    
    return normalized

def prepare_normalized_data(raw_data):
    """
    ëª¨ë“  ë°°ì¹˜ì˜ phaseë¥¼ ì •ê·œí™”ëœ í˜•íƒœë¡œ ë³€í™˜
    """
    normalized_data = []
    
    for batch in raw_data:
        normalized_batch = {
            'batch_id': batch['batch_id'],
            'phase1': normalize_timeseries(batch['phase1']),
            'phase2': normalize_timeseries(batch['phase2']),
            'phase3': normalize_timeseries(batch['phase3']),
            'yield': batch['yield']
        }
        normalized_data.append(normalized_batch)
    
    return normalized_data

# ============================================================================
# 3. íŠ¹ì„± ì¶”ì¶œ (Feature Extraction)
# ============================================================================
def extract_features(phase_data, phase_name):
    """
    ë‹¨ì¼ phaseì—ì„œ ë‹¤ì–‘í•œ íŠ¹ì„± ì¶”ì¶œ
    """
    features = {}
    
    # ê¸°ë³¸ í†µê³„ëŸ‰
    features[f'{phase_name}_mean'] = np.mean(phase_data)
    features[f'{phase_name}_std'] = np.std(phase_data)
    features[f'{phase_name}_min'] = np.min(phase_data)
    features[f'{phase_name}_max'] = np.max(phase_data)
    features[f'{phase_name}_range'] = np.max(phase_data) - np.min(phase_data)
    
    # íŠ¸ë Œë“œ (1ì°¨ ë° 2ì°¨ ë¯¸ë¶„)
    diff1 = np.diff(phase_data)
    diff2 = np.diff(diff1)
    
    features[f'{phase_name}_trend'] = np.mean(diff1)
    features[f'{phase_name}_trend_std'] = np.std(diff1)
    features[f'{phase_name}_acceleration'] = np.mean(diff2)
    
    # ì˜¤ì‹¤ë ˆì´ì…˜ ë¶„ì„
    features[f'{phase_name}_oscillation_amp'] = np.std(diff1)
    
    # FFTë¥¼ í†µí•œ ì£¼íŒŒìˆ˜ ë¶„ì„
    fft_vals = np.abs(fft(phase_data - np.mean(phase_data)))
    freqs = fftfreq(len(phase_data))
    dominant_freq_idx = np.argmax(fft_vals[1:len(fft_vals)//2]) + 1
    features[f'{phase_name}_dominant_freq'] = freqs[dominant_freq_idx]
    features[f'{phase_name}_spectral_power'] = fft_vals[dominant_freq_idx]
    
    # ê·¹ê°’ ë¶„ì„
    peaks = np.where((phase_data[1:-1] > phase_data[:-2]) & 
                     (phase_data[1:-1] > phase_data[2:]))[0] + 1
    valleys = np.where((phase_data[1:-1] < phase_data[:-2]) & 
                       (phase_data[1:-1] < phase_data[2:]))[0] + 1
    
    features[f'{phase_name}_n_peaks'] = len(peaks)
    features[f'{phase_name}_n_valleys'] = len(valleys)
    
    if len(peaks) > 0:
        features[f'{phase_name}_peak_amp'] = np.mean(phase_data[peaks])
        features[f'{phase_name}_peak_variance'] = np.var(phase_data[peaks])
    else:
        features[f'{phase_name}_peak_amp'] = 0
        features[f'{phase_name}_peak_variance'] = 0
    
    # ê³¡ë¥  (Curvature)
    if len(diff2) > 0:
        curvature = np.abs(diff2) / (1 + np.abs(diff1[:-1])**2)**1.5
        features[f'{phase_name}_curvature_mean'] = np.mean(curvature)
    else:
        features[f'{phase_name}_curvature_mean'] = 0
    
    # ì—ë„ˆì§€
    features[f'{phase_name}_energy'] = np.sum(phase_data**2)
    
    return features

def extract_all_features(normalized_data):
    """
    ëª¨ë“  ë°°ì¹˜ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
    """
    feature_list = []
    batch_ids = []
    yields = []
    
    for batch in normalized_data:
        batch_features = {}
        
        for phase_name in ['phase1', 'phase2', 'phase3']:
            phase_features = extract_features(batch[phase_name], phase_name)
            batch_features.update(phase_features)
        
        feature_list.append(batch_features)
        batch_ids.append(batch['batch_id'])
        yields.append(batch['yield'])
    
    features_df = pd.DataFrame(feature_list)
    features_df['batch_id'] = batch_ids
    features_df['yield'] = yields
    
    return features_df

# ============================================================================
# 4. DTW ê±°ë¦¬ ê³„ì‚°
# ============================================================================
def calculate_dtw_distance(phase1, phase2):
    """
    ë‘ ì‹œê³„ì—´ ê°„ì˜ DTW ê±°ë¦¬ ê³„ì‚°
    """
    try:
        return dtw.distance(phase1, phase2)
    except:
        return np.inf

def calculate_batch_similarity_matrix(normalized_data):
    """
    ëª¨ë“  ë°°ì¹˜ ìŒ ê°„ì˜ DTW ê¸°ë°˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    """
    n_batches = len(normalized_data)
    similarity_matrix = np.zeros((n_batches, n_batches))
    
    for i in range(n_batches):
        for j in range(i, n_batches):
            # Phaseë³„ ê±°ë¦¬ì˜ ê°€ì¤‘ í‰ê· 
            d1 = calculate_dtw_distance(normalized_data[i]['phase1'], 
                                        normalized_data[j]['phase1'])
            d2 = calculate_dtw_distance(normalized_data[i]['phase2'], 
                                        normalized_data[j]['phase2'])
            d3 = calculate_dtw_distance(normalized_data[i]['phase3'], 
                                        normalized_data[j]['phase3'])
            
            distance = (d1 + d2 + d3) / 3.0
            similarity_matrix[i, j] = distance
            similarity_matrix[j, i] = distance
    
    return similarity_matrix

# ============================================================================
# 4-1. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
# ============================================================================
def determine_optimal_clusters(features_df, top_features, max_clusters=10):
    """
    Elbow method, Silhouette score, Gap statisticì„ ì´ìš©í•œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
    """
    X = features_df[top_features].values
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    inertias = []
    silhouette_scores = []
    K_range = range(2, max_clusters + 1)
    
    from sklearn.metrics import silhouette_score
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))
    
    # Elbow point ì°¾ê¸° (ì´ì°¨ ë¯¸ë¶„)
    elbow_point = np.argmax(np.diff(inertias, 2)) + 2
    
    # Silhouette score ìµœê³ ì 
    best_silhouette_k = list(K_range)[np.argmax(silhouette_scores)]
    
    print("\n=== ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • ===")
    print(f"Elbow Method: {elbow_point}ê°œ í´ëŸ¬ìŠ¤í„° ì¶”ì²œ")
    print(f"Silhouette Score: {best_silhouette_k}ê°œ í´ëŸ¬ìŠ¤í„° ì¶”ì²œ (ì ìˆ˜: {max(silhouette_scores):.4f})")
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Elbow curve
    ax = axes[0]
    ax.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
    ax.axvline(x=elbow_point, color='red', linestyle='--', linewidth=2, label=f'Elbow: K={elbow_point}')
    ax.set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜ (K)')
    ax.set_ylabel('ê´€ì„± (Inertia)')
    ax.set_title('Elbow Method - ìµœì  K ê²°ì •')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # Silhouette score
    ax = axes[1]
    ax.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)
    ax.axvline(x=best_silhouette_k, color='red', linestyle='--', linewidth=2, 
               label=f'Best: K={best_silhouette_k}')
    ax.set_xlabel('í´ëŸ¬ìŠ¤í„° ìˆ˜ (K)')
    ax.set_ylabel('Silhouette Score')
    ax.set_title('Silhouette Score - ìµœì  K ê²°ì •')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ì¶”ì²œ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ë‘ ë°©ë²•ì˜ í‰ê· )
    optimal_k = int(np.mean([elbow_point, best_silhouette_k]))
    
    return optimal_k, fig

# ============================================================================
# 4-2. Golden Batch ì‹ë³„ (í´ëŸ¬ìŠ¤í„°ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ)
# ============================================================================
def identify_golden_batches_statistical(features_df, yield_threshold_percentile=75):
    """
    ìˆ˜ìœ¨ ê¸°ë°˜ í†µê³„ì  ì„ê³„ê°’ìœ¼ë¡œ Golden batch ì‹ë³„ (í´ëŸ¬ìŠ¤í„° ë¹„ì˜ì¡´ì )
    
    Parameters:
    - yield_threshold_percentile: ìƒìœ„ ëª‡ í¼ì„¼íƒ€ì¼ì„ goldenìœ¼ë¡œ ì •ì˜í•  ê²ƒì¸ê°€
    """
    yield_threshold = np.percentile(features_df['yield'], yield_threshold_percentile)
    golden_mask = features_df['yield'] >= yield_threshold
    
    print(f"\n=== í†µê³„ì  Golden Batch ì‹ë³„ (ìƒìœ„ {100-yield_threshold_percentile}%) ===")
    print(f"ìˆ˜ìœ¨ ì„ê³„ê°’: {yield_threshold:.2f}%")
    print(f"Golden batch ìˆ˜: {golden_mask.sum()}ê°œ ({golden_mask.sum()/len(features_df)*100:.1f}%)")
    print(f"ì¼ë°˜ batch ìˆ˜: {(~golden_mask).sum()}ê°œ")
    
    return golden_mask, yield_threshold

def identify_golden_batches_multimodal(features_df, top_features):
    """
    ë‹¤ì¤‘ ê¸°ì¤€ì„ ì´ìš©í•œ Golden batch ì‹ë³„ (ìˆ˜ìœ¨ + ì•ˆì •ì„± + íŠ¹ì„±)
    """
    from scipy.stats import zscore
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_df[top_features].values)
    
    # 1. ìˆ˜ìœ¨ ê¸°ì¤€ (Z-score > 1)
    yield_zscore = zscore(features_df['yield'].values)
    yield_criterion = yield_zscore > 0.5
    
    # 2. ì•ˆì •ì„± ê¸°ì¤€ (ë³€ë™ì„±ì´ ë‚®ì€ ë°°ì¹˜)
    # phaseë³„ í‘œì¤€í¸ì°¨ê°€ ë‚®ì€ ê²ƒì´ ì¢‹ìŒ
    stability_features = [f for f in top_features if 'std' in f or 'oscillation' in f]
    if stability_features:
        stability_scores = np.mean(features_scaled[:, [list(top_features).index(f) 
                                                        for f in stability_features if f in top_features]], axis=1)
        stability_criterion = stability_scores < np.percentile(stability_scores, 50)
    else:
        stability_criterion = np.ones(len(features_df), dtype=bool)
    
    # 3. íŠ¸ë Œë“œ ê¸°ì¤€ (ì˜ˆìƒëœ ì¶”ì´ë¥¼ ë”°ë¥´ëŠ” ë°°ì¹˜)
    trend_features = [f for f in top_features if 'trend' in f]
    if trend_features:
        trend_indices = [list(top_features).index(f) for f in trend_features if f in top_features]
        trend_scores = np.mean(np.abs(features_scaled[:, trend_indices]), axis=1)
        trend_criterion = trend_scores > np.percentile(trend_scores, 50)
    else:
        trend_criterion = np.ones(len(features_df), dtype=bool)
    
    # í†µí•© ê¸°ì¤€
    golden_mask = yield_criterion & stability_criterion & trend_criterion
    
    print(f"\n=== ë‹¤ì¤‘ ê¸°ì¤€ Golden Batch ì‹ë³„ ===")
    print(f"ìˆ˜ìœ¨ ê¸°ì¤€: {yield_criterion.sum()}ê°œ")
    print(f"ì•ˆì •ì„± ê¸°ì¤€: {stability_criterion.sum()}ê°œ")
    print(f"íŠ¸ë Œë“œ ê¸°ì¤€: {trend_criterion.sum()}ê°œ")
    print(f"í†µí•© (ëª¨ë‘ ë§Œì¡±): {golden_mask.sum()}ê°œ ({golden_mask.sum()/len(features_df)*100:.1f}%)")
    
    return golden_mask

def identify_golden_batches_dbscan(features_df, top_features, eps=0.5, min_samples=3):
    """
    DBSCANì„ ì´ìš©í•œ ë°€ë„ ê¸°ë°˜ Golden batch ì‹ë³„
    - ê³ ìˆ˜ìœ¨ ë°°ì¹˜ë“¤ì˜ ë°€ì§‘ëœ ì˜ì—­ ì°¾ê¸°
    """
    from sklearn.cluster import DBSCAN
    
    X = features_df[top_features].values
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ìˆ˜ìœ¨ ê³ ë ¤ ì¶”ê°€ ì°¨ì› (ì •ê·œí™”)
    yield_scaled = (features_df['yield'].values - features_df['yield'].mean()) / features_df['yield'].std()
    X_with_yield = np.column_stack([X_scaled, yield_scaled])
    
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(X_with_yield)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ í‰ê·  ìˆ˜ìœ¨
    cluster_yields = {}
    for cluster_id in set(clusters):
        if cluster_id == -1:  # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
            continue
        cluster_mask = clusters == cluster_id
        cluster_yields[cluster_id] = features_df[cluster_mask]['yield'].mean()
    
    if cluster_yields:
        golden_cluster_id = max(cluster_yields, key=cluster_yields.get)
        golden_mask = clusters == golden_cluster_id
    else:
        golden_mask = np.zeros(len(features_df), dtype=bool)
    
    print(f"\n=== DBSCAN ê¸°ë°˜ Golden Batch ì‹ë³„ ===")
    print(f"ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(clusters)) - (1 if -1 in clusters else 0)}")
    print(f"ë…¸ì´ì¦ˆ í¬ì¸íŠ¸: {(clusters == -1).sum()}ê°œ")
    print(f"Golden cluster ID: {golden_cluster_id if cluster_yields else 'None'}")
    print(f"Golden batch ìˆ˜: {golden_mask.sum()}ê°œ ({golden_mask.sum()/len(features_df)*100:.1f}%)")
    
    return golden_mask, clusters

def plot_identification_methods(features_df, top_features, mask_statistical, mask_multimodal):
    """
    ë‹¤ì–‘í•œ Golden batch ì‹ë³„ ë°©ë²• ë¹„êµ ì‹œê°í™”
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # PCAë¡œ 2ì°¨ì› ì¶•ì†Œ
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(StandardScaler().fit_transform(features_df[top_features].values))
    
    # 1. ìˆ˜ìœ¨ ë¶„í¬
    ax = axes[0, 0]
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=features_df['yield'].values, 
                        cmap='RdYlGn', s=100, alpha=0.6, edgecolors='black')
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.set_label('ìˆ˜ìœ¨ (%)')
    ax.set_title('PCA ê³µê°„ì˜ ë°°ì¹˜ ë¶„í¬ (ìˆ˜ìœ¨ ê¸°ë°˜ ìƒ‰ìƒ)', fontweight='bold')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    ax.grid(True, alpha=0.3)
    
    # 2. í†µê³„ì  ë°©ë²•
    ax = axes[0, 1]
    colors = ['red' if m else 'lightblue' for m in mask_statistical]
    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, s=100, alpha=0.6, edgecolors='black')
    ax.set_title('í†µê³„ì  ë°©ë²• (ìƒìœ„ 25%)', fontweight='bold')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    ax.grid(True, alpha=0.3)
    
    # 3. ë‹¤ì¤‘ ê¸°ì¤€ ë°©ë²•
    ax = axes[1, 0]
    colors = ['red' if m else 'lightblue' for m in mask_multimodal]
    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, s=100, alpha=0.6, edgecolors='black')
    ax.set_title('ë‹¤ì¤‘ ê¸°ì¤€ ë°©ë²• (ìˆ˜ìœ¨+ì•ˆì •ì„±+íŠ¸ë Œë“œ)', fontweight='bold')
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
    ax.grid(True, alpha=0.3)
    
    # 4. ë°©ë²• ê°„ ì¼ì¹˜ë„
    ax = axes[1, 1]
    agreement = (mask_statistical & mask_multimodal).sum()
    stat_only = (mask_statistical & ~mask_multimodal).sum()
    multi_only = (~mask_statistical & mask_multimodal).sum()
    neither = (~mask_statistical & ~mask_multimodal).sum()
    
    labels = ['Both\nMethods', 'Statistical\nOnly', 'Multi-Criteria\nOnly', 'Neither']
    sizes = [agreement, stat_only, multi_only, neither]
    colors_pie = ['green', 'orange', 'lightcoral', 'lightgray']
    
    ax.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors_pie, startangle=90)
    ax.set_title('ì‹ë³„ ë°©ë²• ê°„ ì¼ì¹˜ë„ ë¹„êµ', fontweight='bold')
    
    plt.tight_layout()
    return fig

# ============================================================================
# 5. ìƒê´€ì„± ë¶„ì„ ë° íŠ¹ì„± ì„ íƒ
# ============================================================================
def analyze_feature_correlation(features_df):
    """
    íŠ¹ì„±ê³¼ ìˆ˜ìœ¨ ê°„ì˜ ìƒê´€ì„± ë¶„ì„
    """
    correlation_results = []
    
    for col in features_df.columns:
        if col not in ['batch_id', 'yield']:
            pearson_corr, p_val = pearsonr(features_df[col], features_df['yield'])
            spearman_corr, sp_val = spearmanr(features_df[col], features_df['yield'])
            
            correlation_results.append({
                'feature': col,
                'pearson_corr': pearson_corr,
                'spearman_corr': spearman_corr,
                'importance': np.abs(pearson_corr)
            })
    
    corr_df = pd.DataFrame(correlation_results)
    corr_df = corr_df.sort_values('importance', ascending=False)
    
    return corr_df

def select_top_features(features_df, corr_df, top_n=15):
    """
    ìƒê´€ì„± ê¸°ë°˜ ìƒìœ„ íŠ¹ì„± ì„ íƒ
    """
    top_features = corr_df.head(top_n)['feature'].tolist()
    return features_df[top_features + ['batch_id', 'yield']]

# ============================================================================
# 6. í´ëŸ¬ìŠ¤í„°ë§ ë° Golden Cycle ì •ì˜
# ============================================================================
def identify_golden_clusters(features_df, top_features, n_clusters=None):
    """
    íŠ¹ì„± ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ê³ ìˆ˜ìœ¨ ê·¸ë£¹ ì‹ë³„
    n_clustersê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ìµœì ê°’ ê²°ì •
    """
    if n_clusters is None:
        n_clusters, _ = determine_optimal_clusters(features_df, top_features)
    
    X = features_df[top_features].values
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    features_df['cluster'] = kmeans.fit_predict(X_scaled)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìˆ˜ìœ¨ í†µê³„
    cluster_stats = features_df.groupby('cluster')['yield'].agg([
        'mean', 'std', 'count'
    ]).sort_values('mean', ascending=False)
    
    print("\n=== í´ëŸ¬ìŠ¤í„°ë³„ ìˆ˜ìœ¨ í†µê³„ ===")
    print(cluster_stats)
    
    # ìµœê³  ìˆ˜ìœ¨ í´ëŸ¬ìŠ¤í„° (Golden Cluster)
    golden_cluster = cluster_stats.index[0]
    
    return features_df, golden_cluster, cluster_stats

def calculate_golden_trajectory(normalized_data, golden_cluster_batches):
    """
    ê³ ìˆ˜ìœ¨ ë°°ì¹˜ë“¤ì˜ í‰ê·  ê¶¤ì  ê³„ì‚°
    """
    golden_traj = {
        'phase1': np.zeros(200),
        'phase2': np.zeros(200),
        'phase3': np.zeros(200)
    }
    
    for phase_name in golden_traj.keys():
        phase_data = np.array([b[phase_name] for b in golden_cluster_batches])
        golden_traj[phase_name] = np.mean(phase_data, axis=0)
    
    return golden_traj

# ============================================================================
# 7. ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
# ============================================================================
def build_yield_prediction_model(features_df, top_features):
    """
    Random Forestë¥¼ ì´ìš©í•œ ìˆ˜ìœ¨ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
    """
    X = features_df[top_features].values
    y = features_df['yield'].values
    
    model = RandomForestRegressor(n_estimators=100, random_state=42, 
                                   max_depth=10, min_samples_split=5)
    model.fit(X, y)
    
    y_pred = model.predict(X)
    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    
    print(f"\n=== ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ===")
    print(f"RÂ² Score: {r2:.4f}")
    print(f"MAE: {mae:.4f}")
    
    # íŠ¹ì„± ì¤‘ìš”ë„
    feature_importance = pd.DataFrame({
        'feature': top_features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return model, feature_importance

# ============================================================================
# 8. ì‹œê°í™”
# ============================================================================
def plot_normalized_batches(normalized_data, golden_cluster_mask, title="Batch Trajectories"):
    """
    ëª¨ë“  ë°°ì¹˜ì˜ ì •ê·œí™”ëœ ê¶¤ì  ì‹œê°í™”
    """
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    phases = ['phase1', 'phase2', 'phase3']
    
    for idx, phase_name in enumerate(phases):
        ax = axes[idx]
        
        for i, batch in enumerate(normalized_data):
            color = 'red' if golden_cluster_mask[i] else 'lightgray'
            alpha = 0.8 if golden_cluster_mask[i] else 0.3
            ax.plot(batch[phase_name], color=color, alpha=alpha, linewidth=1.5)
        
        ax.set_title(f'{phase_name.upper()} - ì •ê·œí™”ëœ ê¶¤ì ')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„')
        ax.set_ylabel('ì¸¡ì •ê°’')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_golden_vs_others(normalized_data, golden_cluster_mask, normalized_data_full):
    """
    Golden cluster vs Others ë¹„êµ
    """
    fig, axes = plt.subplots(2, 3, figsize=(16, 8))
    phases = ['phase1', 'phase2', 'phase3']
    
    for col, phase_name in enumerate(phases):
        golden_data = np.array([normalized_data[i][phase_name] 
                               for i in range(len(normalized_data)) 
                               if golden_cluster_mask[i]])
        other_data = np.array([normalized_data[i][phase_name] 
                              for i in range(len(normalized_data)) 
                              if not golden_cluster_mask[i]])
        
        # í‰ê·  ë° ì‹ ë¢°ë„ êµ¬ê°„
        golden_mean = np.mean(golden_data, axis=0)
        golden_std = np.std(golden_data, axis=0)
        
        other_mean = np.mean(other_data, axis=0) if len(other_data) > 0 else np.zeros(200)
        other_std = np.std(other_data, axis=0) if len(other_data) > 0 else np.zeros(200)
        
        time_axis = np.linspace(0, 1, 200)
        
        # ìƒë‹¨: í‰ê·  ê¶¤ì 
        ax = axes[0, col]
        ax.fill_between(time_axis, golden_mean - golden_std, golden_mean + golden_std,
                        alpha=0.3, color='red', label='Golden (Â±1 std)')
        ax.plot(time_axis, golden_mean, 'r-', linewidth=2, label='Golden Mean')
        
        if len(other_data) > 0:
            ax.fill_between(time_axis, other_mean - other_std, other_mean + other_std,
                           alpha=0.2, color='blue', label='Others (Â±1 std)')
            ax.plot(time_axis, other_mean, 'b--', linewidth=2, label='Others Mean')
        
        ax.set_title(f'{phase_name.upper()} - í‰ê·  ê¶¤ì  ë¹„êµ')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„')
        ax.set_ylabel('ì¸¡ì •ê°’')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # í•˜ë‹¨: í‘œì¤€í¸ì°¨ ë¹„êµ
        ax = axes[1, col]
        ax.plot(time_axis, golden_std, 'r-', linewidth=2, label='Golden Std')
        if len(other_data) > 0:
            ax.plot(time_axis, other_std, 'b--', linewidth=2, label='Others Std')
        ax.set_title(f'{phase_name.upper()} - ì•ˆì •ì„± (í‘œì¤€í¸ì°¨)')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„')
        ax.set_ylabel('í‘œì¤€í¸ì°¨')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_correlation_heatmap(features_df, top_features):
    """
    ìƒìœ„ íŠ¹ì„±ë“¤ ê°„ì˜ ìƒê´€ì„± íˆíŠ¸ë§µ
    """
    corr_matrix = features_df[top_features].corr()
    
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
                center=0, square=True, ax=ax, cbar_kws={'label': 'ìƒê´€ê³„ìˆ˜'})
    ax.set_title('ìƒìœ„ íŠ¹ì„±ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„')
    plt.tight_layout()
    
    return fig

def plot_feature_importance(feature_importance, top_n=15):
    """
    íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    top_features = feature_importance.head(top_n)
    
    ax.barh(range(len(top_features)), top_features['importance'].values, color='steelblue')
    ax.set_yticks(range(len(top_features)))
    ax.set_yticklabels(top_features['feature'].values)
    ax.set_xlabel('ì¤‘ìš”ë„ (Feature Importance)')
    ax.set_title('ìˆ˜ìœ¨ ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” íŠ¹ì„± ì¤‘ìš”ë„')
    plt.tight_layout()
    
    return fig

def plot_golden_trajectory(golden_trajectory, normalized_data, golden_mask):
    """
    Golden Cycleì˜ ìµœì  ì¶”ì´ë¥¼ ìƒì„¸í•˜ê²Œ ì‹œê°í™”
    """
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)
    
    phases = ['phase1', 'phase2', 'phase3']
    phase_names = ['Phase 1\n(ìƒìŠ¹ ì¶”ì„¸)', 'Phase 2\n(ì˜¤ì‹¤ë ˆì´ì…˜)', 'Phase 3\n(í•˜ê°• ì¶”ì„¸)']
    
    time_axis = np.linspace(0, 1, 200)
    
    # ====== ìƒë‹¨: ê° phaseë³„ ìƒì„¸ ë¶„ì„ ======
    for col, (phase_name, phase_label) in enumerate(zip(phases, phase_names)):
        ax = fig.add_subplot(gs[0, col])
        
        # Golden cluster ë°ì´í„°
        golden_data = np.array([normalized_data[i][phase_name] 
                               for i in range(len(normalized_data)) 
                               if golden_mask.iloc[i]])
        
        # ëª¨ë“  ë°°ì¹˜
        all_data = np.array([normalized_data[i][phase_name] 
                            for i in range(len(normalized_data))])
        
        # ê°œë³„ ë°°ì¹˜ (íˆ¬ëª…ë„)
        for trajectory in all_data:
            ax.plot(time_axis, trajectory, 'gray', alpha=0.15, linewidth=0.8)
        
        # Golden ë°°ì¹˜ë“¤
        for trajectory in golden_data:
            ax.plot(time_axis, trajectory, 'lightcoral', alpha=0.4, linewidth=1.2)
        
        # Golden í‰ê·  (êµµì€ ì„ )
        ax.plot(time_axis, golden_trajectory[phase_name], 'r-', linewidth=3.5, 
                label='Golden Mean', zorder=10)
        
        # ì‹ ë¢°ë„ êµ¬ê°„
        golden_std = np.std(golden_data, axis=0)
        ax.fill_between(time_axis, 
                        golden_trajectory[phase_name] - golden_std,
                        golden_trajectory[phase_name] + golden_std,
                        alpha=0.25, color='red', label='Â±1 Std Dev', zorder=5)
        
        ax.set_title(phase_label, fontsize=12, fontweight='bold')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„ ì§„í–‰ë„')
        ax.set_ylabel('ì¸¡ì •ê°’')
        ax.grid(True, alpha=0.3)
        ax.legend(loc='best', fontsize=9)
    
    # ====== ì¤‘ë‹¨: ë¯¸ë¶„ ë¶„ì„ (íŠ¸ë Œë“œ/ì†ë„) ======
    for col, phase_name in enumerate(phases):
        ax = fig.add_subplot(gs[1, col])
        
        golden_data = np.array([normalized_data[i][phase_name] 
                               for i in range(len(normalized_data)) 
                               if golden_mask.iloc[i]])
        
        # 1ì°¨ ë¯¸ë¶„ (ë³€í™” ì†ë„)
        golden_diff = np.diff(golden_trajectory[phase_name])
        golden_diff_std = np.std([np.diff(traj) for traj in golden_data], axis=0)
        
        time_axis_diff = np.linspace(0, 1, len(golden_diff))
        
        # ë°°ê²½ (ëª¨ë“  ë°°ì¹˜)
        for trajectory in golden_data:
            ax.plot(time_axis_diff, np.diff(trajectory), 'lightgray', 
                   alpha=0.3, linewidth=0.8)
        
        # Golden í‰ê·  ë³€í™”ìœ¨
        ax.plot(time_axis_diff, golden_diff, 'darkred', linewidth=2.5, label='Mean Trend')
        
        # ì‹ ë¢°ë„ êµ¬ê°„
        ax.fill_between(time_axis_diff, 
                        golden_diff - golden_diff_std,
                        golden_diff + golden_diff_std,
                        alpha=0.3, color='red', label='Trend Â±1 Std')
        
        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
        ax.set_title(f'{phases[col].upper()}\në³€í™” ì†ë„ (1st Derivative)', 
                    fontsize=11, fontweight='bold')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„ ì§„í–‰ë„')
        ax.set_ylabel('ë³€í™”ìœ¨ (dX/dt)')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
    
    # ====== í•˜ë‹¨: 2ì°¨ ë¯¸ë¶„ (ê°€ì†ë„/ê³¡ë¥ ) ======
    for col, phase_name in enumerate(phases):
        ax = fig.add_subplot(gs[2, col])
        
        # 2ì°¨ ë¯¸ë¶„ (ê°€ì†ë„)
        golden_diff2 = np.diff(np.diff(golden_trajectory[phase_name]))
        
        time_axis_diff2 = np.linspace(0, 1, len(golden_diff2))
        
        # Golden ê°€ì†ë„
        ax.plot(time_axis_diff2, golden_diff2, 'darkred', linewidth=2.5, 
               marker='o', markersize=4, label='Acceleration')
        
        # 0 ê¸°ì¤€ì„ 
        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)
        
        # ì–‘ìˆ˜/ìŒìˆ˜ ì˜ì—­ ê°•ì¡°
        ax.fill_between(time_axis_diff2, 0, golden_diff2, 
                        where=(golden_diff2 >= 0), alpha=0.3, color='green', 
                        label='Positive (Speeding up)')
        ax.fill_between(time_axis_diff2, 0, golden_diff2, 
                        where=(golden_diff2 < 0), alpha=0.3, color='red', 
                        label='Negative (Slowing down)')
        
        ax.set_title(f'{phases[col].upper()}\nê°€ì†ë„ (2nd Derivative)', 
                    fontsize=11, fontweight='bold')
        ax.set_xlabel('ì •ê·œí™”ëœ ì‹œê°„ ì§„í–‰ë„')
        ax.set_ylabel('ê°€ì†ë„ (dÂ²X/dtÂ²)')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
    
    fig.suptitle('Golden Cycle ìµœì  ì¶”ì´ ë¶„ì„\nê°œë³„ ê¶¤ì  | íŠ¸ë Œë“œ | ê°€ì†ë„', 
                fontsize=14, fontweight='bold', y=0.995)
    
    return fig

def plot_golden_trajectory_combined(golden_trajectory, normalized_data, golden_mask, 
                                    features_df_clustered, golden_cluster):
    """
    Golden Cycleì˜ ì „ì²´ í”„ë¡œíŒŒì¼ì„ í•œ ë²ˆì— í‘œì‹œ
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 10))
    
    phases = ['phase1', 'phase2', 'phase3']
    phase_colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    
    # ====== ì¢Œìƒ: ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê¶¤ì  ======
    ax = axes[0, 0]
    
    time_axis = np.linspace(0, 1, 200)
    total_time = np.concatenate([
        time_axis,
        1 + time_axis,
        2 + time_axis
    ])
    
    golden_data = np.array([normalized_data[i]['phase1'] 
                           for i in range(len(normalized_data)) 
                           if golden_mask.iloc[i]])
    
    for trajectory in golden_data:
        full_traj = np.concatenate([
            normalized_data[np.where(golden_mask)[0][0]]['phase1'],
            normalized_data[np.where(golden_mask)[0][0]]['phase2'],
            normalized_data[np.where(golden_mask)[0][0]]['phase3']
        ])
        ax.plot(total_time, full_traj, 'lightgray', alpha=0.2, linewidth=0.8)
    
    # Golden í‰ê·  ì „ì²´ ê¶¤ì 
    full_golden = np.concatenate([
        golden_trajectory['phase1'],
        golden_trajectory['phase2'],
        golden_trajectory['phase3']
    ])
    
    ax.plot(total_time, full_golden, 'r-', linewidth=3, label='Golden Trajectory', zorder=10)
    
    # Phase êµ¬ë¶„ì„ 
    for phase_sep in [1, 2]:
        ax.axvline(x=phase_sep, color='black', linestyle='--', alpha=0.5, linewidth=2)
    
    ax.set_xticks([0.5, 1.5, 2.5])
    ax.set_xticklabels(['Phase 1\nìƒìŠ¹', 'Phase 2\nì˜¤ì‹¤ë ˆì´ì…˜', 'Phase 3\ní•˜ê°•'])
    ax.set_title('ì „ì²´ í”„ë¡œì„¸ìŠ¤ Golden Trajectory', fontsize=12, fontweight='bold')
    ax.set_ylabel('ì¸¡ì •ê°’')
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=10)
    
    # ====== ìš°ìƒ: Phaseë³„ íŠ¹ì„± ë¹„êµ ======
    ax = axes[0, 1]
    
    phase_stats = []
    for i, phase_name in enumerate(phases):
        golden_phase = np.array([normalized_data[j][phase_name] 
                                for j in range(len(normalized_data)) 
                                if golden_mask.iloc[j]])
        
        phase_stats.append({
            'phase': f'Phase {i+1}',
            'mean': np.mean(golden_trajectory[phase_name]),
            'std': np.std(golden_phase),
            'range': np.max(golden_trajectory[phase_name]) - np.min(golden_trajectory[phase_name]),
            'trend': np.mean(np.diff(golden_trajectory[phase_name]))
        })
    
    stat_names = ['mean', 'std', 'range', 'trend']
    x = np.arange(len(phases))
    width = 0.2
    
    for idx, stat_name in enumerate(stat_names):
        values = [stat['std' if stat_name == 'std' else stat_name] for stat in phase_stats]
        ax.bar(x + idx*width, values, width, label=stat_name.upper())
    
    ax.set_xlabel('Phase')
    ax.set_ylabel('ê°’')
    ax.set_title('Phaseë³„ íŠ¹ì„± ë¹„êµ', fontsize=12, fontweight='bold')
    ax.set_xticks(x + width * 1.5)
    ax.set_xticklabels([s['phase'] for s in phase_stats])
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # ====== ì¢Œí•˜: ìˆ˜ìœ¨ ë¶„í¬ ======
    ax = axes[1, 0]
    
    golden_yields = features_df_clustered[golden_mask]['yield'].values
    other_yields = features_df_clustered[~golden_mask]['yield'].values
    
    ax.hist(golden_yields, bins=8, alpha=0.6, color='red', label=f'Golden (Î¼={golden_yields.mean():.1f}%)', edgecolor='darkred')
    ax.hist(other_yields, bins=8, alpha=0.6, color='blue', label=f'Others (Î¼={other_yields.mean():.1f}%)', edgecolor='darkblue')
    
    ax.axvline(golden_yields.mean(), color='red', linestyle='--', linewidth=2)
    ax.axvline(other_yields.mean(), color='blue', linestyle='--', linewidth=2)
    
    ax.set_xlabel('ìˆ˜ìœ¨ (%)')
    ax.set_ylabel('ë°°ì¹˜ ìˆ˜')
    ax.set_title('Golden vs Others ìˆ˜ìœ¨ ë¶„í¬', fontsize=12, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    
    # ====== ìš°í•˜: í†µê³„ ì •ë³´ ======
    ax = axes[1, 1]
    ax.axis('off')
    
    golden_count = golden_mask.sum()
    golden_yield_mean = golden_yields.mean()
    golden_yield_std = golden_yields.std()
    
    stats_text = f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘       GOLDEN CYCLE ìµœì  ì¡°ê±´           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ğŸ“Š ë°°ì¹˜ í†µê³„
    â”œâ”€ Golden Batch ìˆ˜: {golden_count}ê°œ
    â”œâ”€ ì´ Batch ìˆ˜: {len(normalized_data)}ê°œ
    â””â”€ Golden ë¹„ìœ¨: {golden_count/len(normalized_data)*100:.1f}%
    
    ğŸ¯ ìˆ˜ìœ¨ ì„±ëŠ¥
    â”œâ”€ í‰ê·  ìˆ˜ìœ¨: {golden_yield_mean:.2f}%
    â”œâ”€ í‘œì¤€í¸ì°¨: {golden_yield_std:.2f}%
    â”œâ”€ ìµœì†Œê°’: {golden_yields.min():.2f}%
    â””â”€ ìµœëŒ€ê°’: {golden_yields.max():.2f}%
    
    ğŸ“ˆ Phaseë³„ íŠ¹ì„±
    â”œâ”€ Phase 1 ë²”ìœ„: {np.max(golden_trajectory['phase1']) - np.min(golden_trajectory['phase1']):.2f}
    â”œâ”€ Phase 2 í‰ê· : {np.mean(golden_trajectory['phase2']):.2f}
    â””â”€ Phase 3 ê¸°ìš¸ê¸°: {np.mean(np.diff(golden_trajectory['phase3'])):.4f}
    
    âœ“ ê¶Œì¥ ìš´ì „ ì¡°ê±´
    â”œâ”€ Phase 1: ì•ˆì •ì  ìƒìŠ¹ ì¶”ì´ ìœ ì§€
    â”œâ”€ Phase 2: ì œì–´ëœ ì˜¤ì‹¤ë ˆì´ì…˜ (í‘œì¤€í¸ì°¨ ìµœì†Œí™”)
    â””â”€ Phase 3: ì¼ì •í•œ ì†ë„ë¡œ ê°ì†Œ
    """
    
    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes,
           fontsize=10, verticalalignment='top', fontfamily='monospace',
           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    fig.suptitle('Golden Cycle ìµœì  ì¡°ê±´ ì¢…í•© ë¶„ì„', 
                fontsize=14, fontweight='bold', y=0.995)
    
    plt.tight_layout()
    return fig

# ============================================================================
# 9. ë©”ì¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸
# ============================================================================
def main():
    print("=" * 70)
    print("ACN ì •ì œ ë°°ì¹˜ - Golden Cycle ë¶„ì„")
    print("=" * 70)
    
    # 1. ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´)
    print("\n[1ë‹¨ê³„] ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    raw_data = generate_sample_data(n_batches=50)
    normalized_data = prepare_normalized_data(raw_data)
    
    # 2. íŠ¹ì„± ì¶”ì¶œ
    print("[2ë‹¨ê³„] íŠ¹ì„± ì¶”ì¶œ ì¤‘...")
    features_df = extract_all_features(normalized_data)
    print(f"ì¶”ì¶œëœ íŠ¹ì„± ìˆ˜: {len(features_df.columns) - 2}")
    
    # 3. ìƒê´€ì„± ë¶„ì„
    print("[3ë‹¨ê³„] ìƒê´€ì„± ë¶„ì„ ì¤‘...")
    corr_df = analyze_feature_correlation(features_df)
    top_features = select_top_features(features_df, corr_df, top_n=15)
    
    print("\n=== ìƒìœ„ ìƒê´€ íŠ¹ì„± (ìˆ˜ìœ¨ê³¼ì˜ ê´€ê³„) ===")
    print(corr_df.head(15)[['feature', 'pearson_corr']])
    
    # 4-1. ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
    print("\n[4-1ë‹¨ê³„] ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì • ì¤‘...")
    optimal_k, fig_optimal_k = determine_optimal_clusters(features_df, 
                                                          top_features['feature'].tolist())
    
    # 4-2. í´ëŸ¬ìŠ¤í„°ë§ (ê²°ì •ëœ ìµœì  K ì‚¬ìš©)
    print("\n[4-2ë‹¨ê³„] í´ëŸ¬ìŠ¤í„°ë§ ë° Golden Cycle ì‹ë³„ ì¤‘...")
    features_df_clustered, golden_cluster, cluster_stats = identify_golden_clusters(
        features_df, top_features['feature'].tolist(), n_clusters=optimal_k)
    
    # 4-3. ë‹¤ì–‘í•œ Golden batch ì‹ë³„ ë°©ë²• ì ìš©
    print("\n[4-3ë‹¨ê³„] ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ Golden batch ì‹ë³„ ì¤‘...")
    
    # ë°©ë²• 1: í†µê³„ì  ë°©ë²• (ìƒìœ„ 25%)
    golden_mask_statistical, yield_threshold = identify_golden_batches_statistical(
        features_df_clustered, yield_threshold_percentile=75)
    
    # ë°©ë²• 2: ë‹¤ì¤‘ ê¸°ì¤€ (ìˆ˜ìœ¨+ì•ˆì •ì„±+íŠ¸ë Œë“œ)
    golden_mask_multimodal = identify_golden_batches_multimodal(
        features_df_clustered, top_features['feature'].tolist())
    
    # ë°©ë²• 3: DBSCAN ê¸°ë°˜
    golden_mask_dbscan, dbscan_clusters = identify_golden_batches_dbscan(
        features_df_clustered, top_features['feature'].tolist(), eps=1.0, min_samples=3)
    
    # ì‹œê°í™”
    fig_comparison = plot_identification_methods(features_df_clustered, 
                                                  top_features['feature'].tolist(),
                                                  golden_mask_statistical, 
                                                  golden_mask_multimodal)
    
    # ìµœì¢… ê²°ì •: ë‹¤ì¤‘ ê¸°ì¤€ ë°©ë²• ì‚¬ìš© (ê°€ì¥ ì—„ë°€í•¨)
    golden_mask = golden_mask_multimodal
    
    print(f"\nâœ“ ìµœì¢… ì„ íƒ: ë‹¤ì¤‘ ê¸°ì¤€ ë°©ë²• ({golden_mask.sum()}ê°œ ë°°ì¹˜)")
    golden_batch_ids = features_df_clustered[golden_mask]['batch_id'].tolist()
    golden_cluster_batches = [b for b in normalized_data 
                              if b['batch_id'].split('_')[0] + '...' not in golden_batch_ids]
    
    # ì‹¤ì œ ë§¤ì¹­ì„ ìœ„í•´ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±
    golden_cluster_batches = [normalized_data[i] for i in range(len(normalized_data)) 
                              if golden_mask.iloc[i]]
    
    golden_trajectory = calculate_golden_trajectory(normalized_data, golden_cluster_batches)
    
    print(f"\nGolden Cluster ID: {golden_cluster}")
    print(f"Golden Batch ìˆ˜: {len(golden_cluster_batches)}")
    print(f"í‰ê·  ìˆ˜ìœ¨: {features_df_clustered[golden_mask]['yield'].mean():.2f}%")
    
    # 5. ì˜ˆì¸¡ ëª¨ë¸
    print("\n[5ë‹¨ê³„] ìˆ˜ìœ¨ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ì¤‘...")
    model, feature_importance = build_yield_prediction_model(
        features_df_clustered, top_features['feature'].tolist())
    
    print("\n=== ìƒìœ„ ì¤‘ìš” íŠ¹ì„± ===")
    print(feature_importance.head(10))
    
    # 6. ì‹œê°í™”
    print("\n[6ë‹¨ê³„] ì‹œê°í™” ìƒì„± ì¤‘...")
    
    fig1 = plot_normalized_batches(normalized_data, golden_mask.values, 
                                   title="Batch Trajectories - Golden Batches Highlighted")
    fig1.suptitle('ì •ê·œí™”ëœ ë°°ì¹˜ ê¶¤ì  (ë¹¨ê°•: Golden Cycle)', y=1.02)
    
    fig2 = plot_golden_vs_others(normalized_data, golden_mask.values, normalized_data)
    
    fig3 = plot_correlation_heatmap(features_df_clustered, top_features['feature'].tolist())
    
    fig4 = plot_feature_importance(feature_importance, top_n=15)
    
    fig5 = plot_golden_trajectory(golden_trajectory, normalized_data, golden_mask)
    
    fig6 = plot_golden_trajectory_combined(golden_trajectory, normalized_data, golden_mask, 
                                           features_df_clustered, golden_cluster)
    
    # 7. ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    print("\n[Golden Cycle íŠ¹ì„±]")
    golden_features = features_df_clustered[golden_mask][top_features['feature'].tolist()].mean()
    other_features = features_df_clustered[~golden_mask][top_features['feature'].tolist()].mean()
    
    print("\nGolden vs Others ë¹„êµ (ìƒìœ„ 5ê°œ ì¤‘ìš” íŠ¹ì„±):")
    for feat in feature_importance.head(5)['feature'].values:
        g_val = golden_features[feat]
        o_val = other_features[feat]
        diff_pct = ((g_val - o_val) / (np.abs(o_val) + 1e-6)) * 100
        print(f"  {feat}: Golden={g_val:.4f}, Others={o_val:.4f} (ì°¨ì´: {diff_pct:+.1f}%)")
    
    plt.show()
    
    return {
        'normalized_data': normalized_data,
        'features_df': features_df_clustered,
        'model': model,
        'golden_trajectory': golden_trajectory,
        'golden_cluster': golden_cluster,
        'feature_importance': feature_importance
    }

if __name__ == "__main__":
    results = main()
