{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Dependencies and Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf, tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.layers import Dense,Dropout,Input,Reshape,Lambda, GlobalAveragePooling2D, Concatenate, Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "#TPU Configurations\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE_h = 224 \n",
    "IMG_SIZE_w = 224\n",
    "channel = 3\n",
    "BATCH_SIZE = 32*strategy.num_replicas_in_sync\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "test_path=\"./data/test\"\n",
    "train_path=\"./data/train\"\n",
    "\n",
    "test_set = [str(file) for file in Path(test_path).rglob(f'*.png')]\n",
    "train_set = [str(file) for file in Path(train_path).rglob(f'*.png')]\n",
    "\n",
    "images =[]\n",
    "labels = []\n",
    "\n",
    "image_path = []\n",
    "\n",
    "for file in test_set:\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.resize(image, (IMG_SIZE_h,IMG_SIZE_w))\n",
    "    images.append(image)\n",
    "\n",
    "    image_path.append(file)\n",
    "    labels.append(1)\n",
    "    \n",
    "for file in train_set:\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.resize(image, (IMG_SIZE_h,IMG_SIZE_w))\n",
    "\n",
    "    images.append(image)\n",
    "    image_path.append(file)\n",
    "    labels.append(0)\n",
    "\n",
    "imgaes = np.array(images, dtype=np.float32) / 255.0\n",
    "labels = np.array(labels, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(imgaes,np.array(labels),test_size=0.198,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i)Focal Loss + Label Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.25, ls=0.1):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss for binary classification with label smoothing.\n",
    "    Formula:\n",
    "        loss = -alpha*((1-p)^gamma)*log(p)\n",
    "        y_ls = (1 - α) * y_true + α\n",
    "    Parameters:\n",
    "        alpha -- weighting factor for the positive class\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "        ls    -- label smoothing parameter\n",
    "    Default value:\n",
    "        gamma -- 2.0\n",
    "        alpha -- 0.25\n",
    "        ls    -- 0.1\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon to prevent division by zero or log(0)\n",
    "        epsilon = K.epsilon()\n",
    "        \n",
    "        # Apply label smoothing\n",
    "        y_true_ls = (1 - ls) * y_true + ls  # Smoothing label for binary case\n",
    "\n",
    "        # Clip predictions to prevent log(0) and other issues\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true_ls * tf.math.log(y_pred) - (1 - y_true_ls) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Calculate weight factor based on the focal loss formula\n",
    "        weight = alpha * y_true_ls * tf.math.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true_ls) * tf.math.pow(y_pred, gamma)\n",
    "\n",
    "        # Final loss computation\n",
    "        loss = weight * cross_entropy\n",
    "        \n",
    "        # Sum losses for each example in the batch\n",
    "        return tf.reduce_sum(loss, axis=1)\n",
    "\n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "\n",
    "def categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.25,ls=0.1,classes=2):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha*((1-p)^gamma)*log(p)\n",
    "        y_ls = (1 - α) * y_hot    + α / classes\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "        ls    -- label smoothing parameter(alpha)\n",
    "        classes     -- No. of classes\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "        ls    -- 0.1\n",
    "        classes     -- 4\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        #label smoothing\n",
    "        y_pred_ls = (1 - ls) * y_pred + ls / classes\n",
    "        # Clip the prediction value\n",
    "        y_pred_ls = tf.clip_by_value(y_pred_ls, epsilon, 1.0-epsilon)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true*tf.math.log(y_pred_ls)\n",
    "        # Calculate weight that consists of  modulating factor and weighting factor\n",
    "        weight = alpha * y_true * tf.math.pow((1-y_pred_ls), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = tf.math.reduce_sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) BiLinear Layer (outer_product())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def outer_product(x):\n",
    "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
    "    \n",
    "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
    "    \n",
    "    # Divide by feature map size [sizexsize]\n",
    "    size1 = int(x[1].shape[1])\n",
    "    size2 = int(x[1].shape[2])\n",
    "    phi_I = tf.divide(phi_I, size1*size2)\n",
    "    \n",
    "    # Take signed square root of phi_I\n",
    "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
    "    \n",
    "    # Apply l2 normalization\n",
    "    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n",
    "    return z_l2\n",
    "\n",
    "def get_output_shape(input_shape):\n",
    "    # Calculate the output shape based on the outer product operation\n",
    "    # Assuming input_shape is (batch_size, 3584)\n",
    "    return (input_shape[0], input_shape[0][1] * input_shape[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii)F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"f1_score\", **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model():\n",
    "    # Define input\n",
    "    input_shape = (IMG_SIZE_h, IMG_SIZE_w, channel)\n",
    "    input_tensor = Input(shape=(IMG_SIZE_h, IMG_SIZE_w, channel))\n",
    "\n",
    "    # Create EfficientNet backbones\n",
    "    base_model1 = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "    base_model2 = efn.EfficientNetB0(weights='noisy-student', include_top=False, input_shape=input_shape, )\n",
    "\n",
    "    # base_model1 = tf.keras.applications.EfficientNetV2B0 (weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "    # base_model2 = tf.keras.applications.EfficientNetV2B0 (weights='noisy-student', include_top=False, input_shape=input_shape, )\n",
    "    # base_model1 = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "\n",
    "    base_model1.name = \"EfficientNetB0_imagenetWeight\"\n",
    "    base_model2.name = \"EfficientNetB0_noisy-studentWeight\"\n",
    "    for layer in base_model1.layers:\n",
    "        layer.name = 'model1_' + layer.name\n",
    "\n",
    "    for layer in base_model2.layers:\n",
    "        layer.name = 'model2_' + layer.name\n",
    "\n",
    "    x1 = base_model1(input_tensor)\n",
    "    x2 = base_model2(input_tensor)\n",
    "\n",
    "    # Get the output features from both models and apply global pooling\n",
    "    d1 = GlobalAveragePooling2D()(x1)  # This will be shape (batch_size, 1792) for EfficientNetB4\n",
    "    d2 = GlobalAveragePooling2D()(x2)  # This will be shape (batch_size, 1792) for EfficientNetB4\n",
    "\n",
    "    # Instead of outer product, use a simpler approach to combine features\n",
    "    # Option 1: Concatenate features\n",
    "    combined_features = Concatenate()([d1, d2])  # Shape will be (batch_size, 3584)\n",
    "\n",
    "    # Option 2: Element-wise multiplication (if you want interaction between features)\n",
    "    # combined_features = Multiply()([d1, d2])  # Shape will be (batch_size, 1792)\n",
    "\n",
    "    # Add an intermediate dense layer to reduce dimensionality if needed\n",
    "    intermediate = Dense(512, activation='relu')(combined_features)\n",
    "\n",
    "    # Final prediction layer\n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(intermediate)\n",
    "\n",
    "    # Create the full model\n",
    "    model = Model(inputs=input_tensor, outputs=predictions)\n",
    "\n",
    "    # Now compile the combined model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003, decay=1e-3),\n",
    "        loss=binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def outer_product(x):\n",
    "    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n",
    "    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n",
    "    \n",
    "    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n",
    "    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n",
    "    \n",
    "    # Divide by feature map size [sizexsize]\n",
    "    size1 = int(x[1].shape[1])\n",
    "    size2 = int(x[1].shape[2])\n",
    "    phi_I = tf.divide(phi_I, size1*size2)\n",
    "    \n",
    "    # Take signed square root of phi_I\n",
    "    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n",
    "    \n",
    "    # Apply l2 normalization\n",
    "    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n",
    "    return z_l2\n",
    "\n",
    "def get_model():\n",
    "    # Define input\n",
    "    input_shape = (IMG_SIZE_h, IMG_SIZE_w, channel)\n",
    "    input_tensor = Input(shape=(IMG_SIZE_h, IMG_SIZE_w, channel))\n",
    "\n",
    "    # Create EfficientNet backbones\n",
    "    base_model1 = efn.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "    base_model2 = efn.EfficientNetB0(weights='noisy-student', include_top=False, input_shape=input_shape, )\n",
    "\n",
    "    # base_model1 = tf.keras.applications.EfficientNetV2B0 (weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "    # base_model2 = tf.keras.applications.EfficientNetV2B0 (weights='noisy-student', include_top=False, input_shape=input_shape, )\n",
    "    # base_model1 = tf.keras.applications.EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=input_shape, )\n",
    "\n",
    "    base_model1.name = \"EfficientNetB0_imagenetWeight\"\n",
    "    base_model2.name = \"EfficientNetB0_noisy-studentWeight\"\n",
    "    for layer in base_model1.layers:\n",
    "        layer.name = 'model1_' + layer.name\n",
    "\n",
    "    for layer in base_model2.layers:\n",
    "        layer.name = 'model2_' + layer.name\n",
    "\n",
    "    # base_model1 = Model(inputs = input_tensor, outputs= base_model1.output)\n",
    "    # base_model2 = Model(inputs = input_tensor, outputs= base_model2.output )\n",
    "\n",
    "    base_model1(input_tensor)\n",
    "    base_model2(input_tensor)\n",
    "\n",
    "\n",
    "    base_model1.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003, decay=1e-3),\n",
    "        loss=binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125)\n",
    "    ) \n",
    "    base_model2.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003, decay=1e-3),\n",
    "        loss=binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125)\n",
    "    ) \n",
    "\n",
    "    d1 = base_model1.output  # This will be shape (batch_size, 1792) for EfficientNetB4\n",
    "    d2 = base_model2.output\n",
    "    \n",
    "    combined_features = Concatenate()([d1, d2])\n",
    "\n",
    "    bilinear = Lambda(outer_product, name='outer_product1', output_shape=(None, 128*128))(combined_features)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(bilinear)\n",
    "    model = Model(inputs=input_tensor, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def outer_product(inputs):\n",
    "    # Unpack the inputs\n",
    "    x1, x2 = inputs\n",
    "    \n",
    "    # Get shapes\n",
    "    batch_size = tf.shape(x1)[0]\n",
    "    height = tf.shape(x1)[1]\n",
    "    width = tf.shape(x1)[2]\n",
    "    depth1 = x1.shape[3]\n",
    "    depth2 = x2.shape[3]\n",
    "    \n",
    "    # Reshape tensors to 2D\n",
    "    x1_flat = tf.reshape(x1, [batch_size * height * width, depth1])\n",
    "    x2_flat = tf.reshape(x2, [batch_size * height * width, depth2])\n",
    "    \n",
    "    # Reshape to 3D tensors\n",
    "    x1_3d = tf.reshape(x1_flat, [batch_size, height * width, depth1])\n",
    "    x2_3d = tf.reshape(x2_flat, [batch_size, height * width, depth2])\n",
    "    \n",
    "    # Compute outer product using batch matrix multiplication\n",
    "    phi_I = tf.matmul(tf.transpose(x1_3d, [0, 2, 1]), x2_3d)  # [batch, depth1, depth2]\n",
    "    \n",
    "    # Reshape to 2D\n",
    "    phi_I = tf.reshape(phi_I, [batch_size, depth1 * depth2])\n",
    "    \n",
    "    # Normalize by feature map size\n",
    "    phi_I = phi_I / tf.cast(height * width, tf.float32)\n",
    "    \n",
    "    # Signed square root\n",
    "    y_ssqrt = tf.sign(phi_I) * tf.sqrt(tf.abs(phi_I) + 1e-12)\n",
    "    \n",
    "    # L2 normalization\n",
    "    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n",
    "    \n",
    "    return z_l2\n",
    "\n",
    "def get_model():\n",
    "    # Define input\n",
    "    IMG_SIZE_h = 224\n",
    "    IMG_SIZE_w = 224\n",
    "    channel = 3\n",
    "    \n",
    "    input_tensor = Input(shape=(IMG_SIZE_h, IMG_SIZE_w, channel))\n",
    "    \n",
    "    # Create EfficientNet backbones\n",
    "    base_model1 = efn.EfficientNetB0(weights='imagenet', include_top=False)\n",
    "    base_model2 = efn.EfficientNetB0(weights='noisy-student', include_top=False)\n",
    "    \n",
    "    base_model1.name = \"EfficientNetB0_imagenetWeight\"\n",
    "    base_model2.name = \"EfficientNetB0_noisy-studentWeight\"\n",
    "    \n",
    "    # Rename layers to avoid name conflicts\n",
    "    for layer in base_model1.layers:\n",
    "        layer.name = 'model1_' + layer.name\n",
    "    for layer in base_model2.layers:\n",
    "        layer.name = 'model2_' + layer.name\n",
    "    \n",
    "    # Get outputs from each model\n",
    "    d1 = base_model1(input_tensor)\n",
    "    d2 = base_model2(input_tensor)\n",
    "    \n",
    "    # Apply bilinear pooling\n",
    "    bilinear = Lambda(outer_product)([d1, d2])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    predictions = Dense(1, activation='sigmoid', name='predictions')(bilinear)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003, decay=1e-3),\n",
    "        loss=binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.0003, decay=1e-3)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0003, decay=1e-3), \n",
    "    loss=binary_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125),\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "        # keras.metrics.F1Score(name=\"f1score\"),\n",
    "        F1Score\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\keras\\src\\models\\model.py:259\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummary\u001b[39m(\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m     layer_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    227\u001b[0m ):\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m        ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m     \u001b[43msummary_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mline_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mline_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_nested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_nested\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\keras\\src\\utils\\summary_utils.py:361\u001b[0m, in \u001b[0;36mprint_summary\u001b[1;34m(model, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# Print the to the console.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m console\u001b[38;5;241m.\u001b[39mprint(bold_text(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrich\u001b[38;5;241m.\u001b[39mmarkup\u001b[38;5;241m.\u001b[39mescape(model\u001b[38;5;241m.\u001b[39mname)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 361\u001b[0m \u001b[43mconsole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m console\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    363\u001b[0m     bold_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Total params: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;241m+\u001b[39m highlight_number(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreadable_memory_size(total_memory_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m )\n\u001b[0;32m    367\u001b[0m console\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    368\u001b[0m     bold_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Trainable params: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;241m+\u001b[39m highlight_number(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreadable_memory_size(trainable_memory_size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\console.py:1678\u001b[0m, in \u001b[0;36mConsole.print\u001b[1;34m(self, sep, end, style, justify, overflow, no_wrap, emoji, markup, highlight, width, height, crop, soft_wrap, new_line_start, *objects)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     crop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m render_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_hooks[:]\n\u001b[1;32m-> 1678\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m   1679\u001b[0m     renderables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_renderables(\n\u001b[0;32m   1680\u001b[0m         objects,\n\u001b[0;32m   1681\u001b[0m         sep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1686\u001b[0m         highlight\u001b[38;5;241m=\u001b[39mhighlight,\n\u001b[0;32m   1687\u001b[0m     )\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m render_hooks:\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\console.py:864\u001b[0m, in \u001b[0;36mConsole.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exit buffer context.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exit_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\console.py:822\u001b[0m, in \u001b[0;36mConsole._exit_buffer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Leave buffer context, and render content if required.\"\"\"\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_index \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 822\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\console.py:2019\u001b[0m, in \u001b[0;36mConsole._check_buffer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2019\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n\u001b[0;32m   2021\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_broken_pipe()\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\console.py:2035\u001b[0m, in \u001b[0;36mConsole._write_buffer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_jupyter:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjupyter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m-> 2035\u001b[0m     \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2036\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m   2037\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\rich\\jupyter.py:91\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(segments, text)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display \u001b[38;5;28;01mas\u001b[39;00m ipython_display\n\u001b[1;32m---> 91\u001b[0m     \u001b[43mipython_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjupyter_renderable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Handle the case where the Console has force_jupyter=True,\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# but IPython is not installed.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\IPython\\core\\display_functions.py:305\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadata:\n\u001b[0;32m    303\u001b[0m             \u001b[38;5;66;03m# kwarg-specified metadata gets precedence\u001b[39;00m\n\u001b[0;32m    304\u001b[0m             _merge(md_dict, metadata)\n\u001b[1;32m--> 305\u001b[0m         publish_display_data(data\u001b[38;5;241m=\u001b[39mformat_dict, metadata\u001b[38;5;241m=\u001b[39mmd_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display_id:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisplayHandle(display_id)\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\IPython\\core\\display_functions.py:93\u001b[0m, in \u001b[0;36mpublish_display_data\u001b[1;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transient:\n\u001b[0;32m     91\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransient\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m transient\n\u001b[1;32m---> 93\u001b[0m display_pub\u001b[38;5;241m.\u001b[39mpublish(\n\u001b[0;32m     94\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m     95\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     97\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\ipykernel\\zmqshell.py:130\u001b[0m, in \u001b[0;36mZMQDisplayPublisher.publish\u001b[1;34m(self, data, metadata, transient, update)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# type:ignore[unreachable]\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mident\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\jupyter_client\\session.py:852\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, stream, msg_or_type, content, parent, ident, buffers, track, header, metadata)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_version:\n\u001b[0;32m    851\u001b[0m     msg \u001b[38;5;241m=\u001b[39m adapt(msg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_version)\n\u001b[1;32m--> 852\u001b[0m to_send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mident\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m to_send\u001b[38;5;241m.\u001b[39mextend(buffers)\n\u001b[0;32m    854\u001b[0m longest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m to_send])\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\jupyter_client\\session.py:747\u001b[0m, in \u001b[0;36mSession.serialize\u001b[1;34m(self, msg, ident)\u001b[0m\n\u001b[0;32m    744\u001b[0m     to_send\u001b[38;5;241m.\u001b[39mappend(ident)\n\u001b[0;32m    745\u001b[0m to_send\u001b[38;5;241m.\u001b[39mappend(DELIM)\n\u001b[1;32m--> 747\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    748\u001b[0m to_send\u001b[38;5;241m.\u001b[39mappend(signature)\n\u001b[0;32m    750\u001b[0m to_send\u001b[38;5;241m.\u001b[39mextend(real_message)\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\jupyter_client\\session.py:687\u001b[0m, in \u001b[0;36mSession.sign\u001b[1;34m(self, msg_list)\u001b[0m\n\u001b[0;32m    685\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m msg_list:\n\u001b[1;32m--> 687\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\u001b[38;5;241m.\u001b[39mhexdigest()\u001b[38;5;241m.\u001b[39mencode()\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\hmac.py:120\u001b[0m, in \u001b[0;36mHMAC.update\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data from msg into this hashing object.\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hmac \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner\n\u001b[1;32m--> 120\u001b[0m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - accuracy: 0.4369 - f1_score: 0.4355 - loss: 0.0739 - precision: 0.3120 - recall: 0.7480 - val_accuracy: 0.2586 - val_f1_score: 0.4110 - val_loss: 0.0720 - val_precision: 0.2586 - val_recall: 1.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.8520 - f1_score: 0.7791 - loss: 0.0492 - precision: 0.6936 - recall: 0.8957 - val_accuracy: 0.3103 - val_f1_score: 0.4286 - val_loss: 0.0691 - val_precision: 0.2727 - val_recall: 1.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.9487 - f1_score: 0.9033 - loss: 0.0326 - precision: 0.8674 - recall: 0.9482 - val_accuracy: 0.8103 - val_f1_score: 0.4762 - val_loss: 0.0689 - val_precision: 0.8333 - val_recall: 0.3333\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.9405 - f1_score: 0.8946 - loss: 0.0253 - precision: 0.8183 - recall: 0.9903 - val_accuracy: 0.7931 - val_f1_score: 0.5000 - val_loss: 0.0684 - val_precision: 0.6667 - val_recall: 0.4000\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5s/step - accuracy: 0.9746 - f1_score: 0.9583 - loss: 0.0262 - precision: 0.9553 - recall: 0.9625 - val_accuracy: 0.6379 - val_f1_score: 0.4615 - val_loss: 0.0669 - val_precision: 0.3750 - val_recall: 0.6000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = x_train,\n",
    "                    y = y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val)\n",
    "                    )\n",
    "#it will take some time to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print ('Matplotlib version: ', mpl.__version__) # >= 2.0.0\n",
    "\n",
    "val_f1 = history.history['val_f1']\n",
    "f1 = history.history['f1']\n",
    "epochs = range(len(f1))\n",
    "\n",
    "df_categorical_accuracy = pd.DataFrame(val_f1, columns = ['val_f1']) \n",
    "df_f1 = pd.DataFrame(f1, columns = ['f1'])\n",
    "\n",
    "df_categorical_accuracy.to_csv('val_f1.csv')\n",
    "df_f1.to_csv('f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_46      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ EfficientNetB4_noi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,816</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnet-b4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,816</span> │ input_layer_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ EfficientNetB4_n… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ efficientnet-b4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_15      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3584</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,835,520</span> │ concatenate_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ predictions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │ dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_46      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ EfficientNetB4_noi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │ \u001b[38;5;34m17,673,816\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1792\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ efficientnet-b4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m,      │ \u001b[38;5;34m17,673,816\u001b[0m │ input_layer_46[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1792\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ EfficientNetB4_n… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ efficientnet-b4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_15      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3584\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m1,835,520\u001b[0m │ concatenate_15[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ predictions (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m513\u001b[0m │ dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">111,050,197</span> (423.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m111,050,197\u001b[0m (423.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,933,265</span> (140.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,933,265\u001b[0m (140.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250,400</span> (978.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m250,400\u001b[0m (978.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">73,866,532</span> (281.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m73,866,532\u001b[0m (281.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,4)) # set the size that you'd like (width, height)\n",
    "plt.title('F1 Score')\n",
    "plt.ylabel('f1 score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(epochs,val_f1,label='Validation F1 Score')\n",
    "plt.plot(epochs, f1,label='Training F1 Score')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.savefig('F1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Testing and Saving Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path='../input/plant-pathology-2020-fgvc7/'\n",
    "\n",
    "test = pd.read_csv(path+'test.csv')\n",
    "test_id = test['image_id']\n",
    "\n",
    "root = 'images'\n",
    "x_test = [(os.path.join(GCS_DS_PATH,root,idee+'.jpg')) for idee in test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_dataset,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_results(y_pred):\n",
    "    \n",
    "    path='../input/plant-pathology-2020-fgvc7/'\n",
    "    test = pd.read_csv(path + 'test.csv')\n",
    "    test_id = test['image_id']\n",
    "\n",
    "    res = pd.read_csv(path+'train.csv')\n",
    "    res['image_id'] = test_id\n",
    "  \n",
    "    labels = res.keys()\n",
    "\n",
    "    for i in range(1,5):\n",
    "        res[labels[i]] = y_pred[:,i-1]\n",
    "\n",
    "    res.to_csv('submission.csv',index=False)\n",
    "  \n",
    "    print(res.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_results(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"Model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_from_json\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# load json and create model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModel.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m loaded_model_json \u001b[38;5;241m=\u001b[39m json_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m json_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\labq001\\anaconda3\\envs\\efficientnet\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Model.json'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('Model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"Model.h5\")\n",
    "# loaded_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficientnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
