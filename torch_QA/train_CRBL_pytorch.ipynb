{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CRBL Anomaly Detection - PyTorch Training\n",
        "\n",
        "이 노트북은 TensorFlow/Keras로 작성된 CRBL 이상 탐지 모델을 PyTorch로 이식한 학습 코드입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our custom modules\n",
        "from model_pytorch import create_model, count_parameters\n",
        "from loadDataset_generator_pytorch import load_data\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "model_save_name = \"CRBL_250328_pytorch\"\n",
        "learning_rate = 0.0001 \n",
        "weight_decay = 1e-6\n",
        "label_smoothing = 0.125\n",
        "\n",
        "num_classes = 1  # Binary classification\n",
        "input_dim = 128\n",
        "channel = 3 \n",
        "batch_size = 32 \n",
        "input_shape = (channel, input_dim, input_dim)\n",
        "\n",
        "class0_ratio = 0.5 \n",
        "c0 = 0.88\n",
        "c1 = 1 - c0\n",
        "class_weights = torch.tensor([(c1+c0)/(2*c0), (c1+c0)/(2*c1)], dtype=torch.float32).to(device)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10\n",
        "min_delta = 1e-6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_loader, val_loader = load_data(\n",
        "    input_dim=input_dim,\n",
        "    datadir=\"./data/images\",\n",
        "    batch_size=batch_size,\n",
        "    class0_ratio=class0_ratio,\n",
        "    test_csv_path=\"./data/csv/test_CRBL.csv\",\n",
        "    image_dir=\"./data/images\",\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training data\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, (images, labels) in enumerate(train_loader):\n",
        "    for j in range(min(len(images), 10)):\n",
        "        plt.subplot(5, 5, j + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        # Convert tensor to numpy for visualization\n",
        "        img = images[j].permute(1, 2, 0).numpy()\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'Label: {labels[j].item()}')\n",
        "    break\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "print(\"Creating model...\")\n",
        "model = create_model(input_shape=input_shape, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model parameters: {count_parameters(model):,}\")\n",
        "\n",
        "# Loss function with class weights\n",
        "criterion = nn.BCELoss(weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping utility\"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = model.state_dict().copy()\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.squeeze(), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        pred = (output.squeeze() > 0.5).float()\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "    \n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output.squeeze(), target)\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            pred = (output.squeeze() > 0.5).float()\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(30):  # Max 30 epochs\n",
        "    print(f\"\\nEpoch {epoch+1}/30\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Store history\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    # Early stopping check\n",
        "    if early_stopping(val_loss, model):\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model\n",
        "os.makedirs(\"./weights\", exist_ok=True)\n",
        "torch.save(model.state_dict(), f\"./weights/{model_save_name}.pth\")\n",
        "print(f\"Model saved to ./weights/{model_save_name}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accs, label='Train Acc')\n",
        "plt.plot(val_accs, label='Val Acc')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot([optimizer.param_groups[0]['lr']] * len(train_losses))\n",
        "plt.title('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"./weights/{model_save_name}_training_history.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Training completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
